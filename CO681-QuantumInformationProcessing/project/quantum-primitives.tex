\section{Quantum Primitives}\label{sec:quantum-primitives}
\paragraph{}
In this section we present important quantum primitives to be used in our optimization algorithms in later sections. The centerpiece of these algorithms is Grover's search, first presented in \cite{grover1996fast} and rigorously analyzed in \cite{boyer1996tight}, whose presentation we will follow.
\subsection{Grover's Search}
\paragraph{}
Let $T$ be  a table with with $N$ elements, where the $i$-th element is denoted $T[i]$. In a general {\it search problem} we are given $T$ and some $x$ in the universe from which the elements of $T$ are drawn, and are to find some $i \in \{0, \dots, N-1\}$ such that $T[i] = x$. Let $t= |\{ i \in \{0,\dots, N-1\} : T[i] = x\}$. Classically, it is easy to see that when $T$ is unsorted any algorithm which solves this problem with success probability at least $1/2$ will need to make at least $N/2t$ queries to $T$. In contrast, Grover's Quantum Search algorithm will need only $O(\sqrt{N/t})$ queries to have $1/2$ success probability. This is within a constant factor of optimal for a quantum algorithm 
\paragraph{}
Let $A = \{ i : T[i] = x\}$ and let $B = \{ i : T[i] \neq x \}$. Let $k, \ell \in \R$ satisfying $tk^2 + (N-t)\ell^2 = 1$.  We define the following general quantum register state
$$\ket{\Phi(k,\ell)} = \sum_{i \in A} k \ket{i} + \sum{i \in B} \ell \ket{i}.$$
Grover's Search begins with the state $$\ket{\Phi_0} := \sum_i \frac{1}{\sqrt{N}} \ket{i}$$
and each iteration applies a transformation sending some $\ket{\Phi(k,\ell)}$ to
$$\ket{\Phi(\frac{N-2t}{N}k + \frac{2(N-t)}{N}\ell, \frac{N-2t}{N}\ell - \frac{2t}{N} k)}.$$
The transformation is specified as follows (we assume $N$ is a power of $2$ for simplicity, full details appear in the source material). Define for any $K \subseteq \{0, \dots, N-1\}$ the ``conditional phase shift transform" satisfying
$$S_K\ket{i} = \begin{cases}
-\ket{i}, &\text{if $i \in K$} \\
\ket{i}, &\text{otherwise}.
\end{cases}$$
Let $H$ be the Hadamard transform satisfying
$$H\ket{j} = \frac{1}{\sqrt{N}} \sum_i (-1)^{i\cdot j} \ket{i}.$$
Then we observe that the transformation
$$-HS_{\{0\}}HS_A$$
efficiently implements a Grover Search iteration using only $1$ query to $T$.
\paragraph{}
The search begins by fixing some $\lambda \in (1,4/3)$ and initializing $m=1$. Then repeating the following procedure:
\begin{enumerate}
\item Choose $j$ uniformly at random from $\{1, \dots, m\}$.
\item Apply $j$ Grover iterations, as described above, starting from $\ket{\Phi_0}$.
\item Measure and let $i$ be the outcome.
\item If $T[i] = x$ return $i$; otherwise set $m$ to $\min(\lambda m, \sqrt{N})$.
\end{enumerate}
\begin{theorem}\label{th:grovers}
Grover's Search Algorithm above finds a solution in expected time $O(\sqrt{N/t})$. (Alternatively, the algorithm has small failure probability if terminated after $\Omega(\sqrt{N/t})$ iterations)
\end{theorem}
\begin{proof}
The case where $t=0$ can be treated with an appropriate timeout, with arbitrarily small failure probability. The case $t > \frac{3}{4}N$ can be dismissed via classical sampling techniques in constant expected time. So we may assume $1\leq t\leq \frac{3}{4}N$.
\paragraph{}
Let choose $\theta$ so that $\sin^2\theta = \frac{t}{N}$. Since $t\leq \frac{3}{4}N$, if we let $m_0 = \frac{1}{sin(2\theta)}$ then
$$m_0 = \frac{N}{2\sqrt{(N-t)t}} < \sqrt{\frac{N}{t}}.$$
Let $c = \ceil{\log_\lambda m_0}$. Observe that the algorithm sets $m =\lambda^{s-1}$ in iteration $s \leq \frac{3}{4}N$ of the main loop, and since $j$ is chosen randomly, the algorithm uses at most $\frac{1}{2} \lambda^{s-1}$ Grover iterations in expectation at iteration $s$. 
\paragraph{}
To reach iteration $c$ the algorithm would need
$$\frac{1}{2}\sum_{s=1}^{\ceil{\log_\lambda m_0}} \lambda^{s-1} <\frac{1}{2}\frac{\lambda}{\lambda - 1} m_0 = 3m_0$$
Grover iterations in expectation. Since $m_0 = O(\sqrt{\frac{N}{t}})$ if the algorithm terminates before iteration $c$ then we have the claimed running time bound. Notice that clearly the number of Grover iterations dominates the complexity of the running time.
\paragraph{}
Thus we may assume that algorithm uses at least $c$ iterations. The probability of success for $j$ Grover iterations from $\ket{\Phi_0}$ is $tk_j^2= \sin^2((2j+1)\theta)$. Since $j$ is chosen randomly the average success probability of step $2$ is
$$\sum_{j=0}^{m-1}\frac{1}{m}\sin^2((2j+1)\theta) = \frac{1}{2m}\sum_{j=0}^{m-1}1 - \cos((2j+1)\theta) = \frac{1}{2} - \frac{\sin(4m\theta)}{4m\sin(2\theta)}$$
recalling (learning?) that the last equality follows from a trigonometric identity. Since $m \geq \frac{1}{\sin(2\theta)}$ after the algorithm passes $c$ iterations, the above probability is at least $\frac{1}{4}$. Therefore the expected number of Grover iterations after the critical stage is
$$ \frac{1}{2}\sum_{s=0}^\infty \frac{3^s}{4^{s+1}}\lambda^{s+c} < \frac{\lambda}{8-6\lambda}m_0 = \frac{3}{2} m_0. $$
So the algorithm uses at most $3m_0 + \frac{3}{2}m_0 = O(\sqrt{\frac{N}{t}})$ Grover iterations in expectation.
\end{proof}
\subsection{Minima Finding}
\documentclass[letterpaper,12pt,oneside,onecolumn]{article}
\usepackage[margin=1in, bottom=1in, top=1in]{geometry} %1 inch margins
\usepackage{amsmath, amssymb, amstext}
\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{theorem}
\usepackage{tikz}
\usepackage{tkz-berge}

%Macros
\newcommand{\A}{\mathbb{A}} \newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}} \newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}} \newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
 
 
\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\newenvironment{proof}{{\bf Proof:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofof}[1]{{\bf Proof of #1:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofofnobox}[1]{{\bf#1:  }}{}\newenvironment{example}{{\bf Example:  }}{\hfill\rule{2mm}{2mm}}

%\renewcommand{\thesection}{\lecnum.\arabic{section}}
%\renewcommand{\theequation}{\thesection.\arabic{equation}}
%\renewcommand{\thefigure}{\thesection.\arabic{figure}}

\newtheorem{fact}{Fact}[section]
\newtheorem{lemma}[fact]{Lemma}
\newtheorem{theorem}[fact]{Theorem}
\newtheorem{definition}[fact]{Definition}
\newtheorem{corollary}[fact]{Corollary}
\newtheorem{proposition}[fact]{Proposition}
\newtheorem{claim}[fact]{Claim}
\newtheorem{exercise}[fact]{Exercise}
\newtheorem{note}[fact]{Note}
\newtheorem{conjecture}[fact]{Conjecture}

\newcommand{\size}[1]{\ensuremath{\left|#1\right|}}
\newcommand{\ceil}[1]{\ensuremath{\left\lceil#1\right\rceil}}
\newcommand{\floor}[1]{\ensuremath{\left\lfloor#1\right\rfloor}}

%END MACROS
%Page style
\pagestyle{fancy}

\listfiles

\raggedbottom

\lhead{2017-03-14}
\rhead{W. Justin Toth CO750-Approximation Algorithms Assignment 2} %CHANGE n to ASSIGNMENT NUMBER ijk TO COURSE CODE
\renewcommand{\headrulewidth}{1pt} %heading underlined
%\renewcommand{\baselinestretch}{1.2} % 1.2 line spacing for legibility (optional)

\begin{document}

%Question 1
\section{}
\paragraph{}
Let $G = (V,E)$ be an undirected graph with edge cost $c: E\rightarrow \R_+$. Consider the problem $(IP)$:
\begin{align*}
\min\ \sum_{e\in E} c(e) x_e&\\
\text{s.t.}\  \sum_{e\in\delta(S)} x_e &\geq f(S) &\forall S \subseteq V \\
x&\geq 0\\
x&\in \Z^{|E|},
\end{align*}
where $f : S \rightarrow \{0,1\}$ satisfies the maximality property:
$$\text{ for all disjoint } A,B\subseteq V, \text{ we have } f(A \cup B ) \leq \max\{f(A), f(B)\},$$
and $f$ satisfies that there does not exist $S \subseteq V$ and $A, B \subseteq S$ such that
\begin{equation}\label{eq:1}
A\cap B = \emptyset,\ f(S)=f(A)=f(B)=0 \text{ and } f(S\backslash A) = f(S\backslash B) = 1.
\end{equation}
We may assume that $f(V') = 0$, for the every connected component vertex set $V'$, otherwise $(IP)$ is infeasible. Let $(LP)$ denote the linear programming relaxation of $(IP)$ and let $(D)$ denote the dual of $(LP)$:
\begin{align*}
\max\ \sum_{S\subseteq V} f(S) y_S&\\
\text{s.t.}\ \sum_{S : e \in \delta(S)} y_S &\leq c(e) &\forall e \in E \\
y&\geq 0. 
\end{align*}
\paragraph{Algorithm}
Consider the primal-dual algorithm, $\cA$, for this problem:
\begin{enumerate}
\item Start with $x=0$, $y=0$, and $F \rightarrow \emptyset$.
\item While $F$ is not feasible do:
	\begin{enumerate}
	\item Increase $y_S$ uniformly for ``minimal violated cuts" with respect to $F$ until some dual constraint corresponding to an edge $e$ becomes tight.
	\item Set $x_e = 1$, $F = F\cup \{e\}$.
	\end{enumerate}
\item Reverse Deletion: Consider all edges $e$ in $F$ in reverse of the order they were added to $F$ and if $F\backslash \{e\}$ is feasible then set $F = F\backslash\{e\}$ and $x_e = 0$.
\end{enumerate}
\paragraph{}
We intend to show that $\cA$ is a polynomial time $2$-approximation algorithm for $(IP)$. We will need the following lemma from class:
\begin{lemma}\label{lemma:maximality}
Let $f$ be a $01$-function satisfying maximality. Let $F\subseteq E$. Then
\begin{enumerate}
\item $F$ is feasible for $(IP)$ with $f$ if and only if every connected component $C$ of $(V,F)$ satisfies $f(C) = 0$.
\item The minimal violated cuts are connected components $C$ that have $f(C) = 1$.
\end{enumerate}
\end{lemma}
This lemma allows us to show that $\cA$ terminates in polynomial time.
\begin{lemma}\label{lemma:poly1}
Suppose we have a polynomial time oracle for $f$. Then $\cA$ returns a feasible solution for $(IP)$ in polynomial time
\end{lemma}
\begin{proof}
By definition of $\cA$ if $\cA$ terminates then the result is feasible. To see that $\cA$ terminates observe that in every iteration of step $2$ some edge $e \in |E|$ is added to $F$. Since $E$ is feasible, the while loop terminates after $O(|E|)$ iterations. Hence $\cA$ terminates, and returns a feasible solution.
\paragraph{} 
It remains to show that $\cA$ runs in polynomial time. Since we have a polynomial time oracle for $f$ it is easy to see that step $3$ runs in polynomial time. Further we need only maintain $y_S$ for non-zero $y_S$, of which we will show there are a polynomial number. We need to verify step $2(a)$ can be done in polynomial time. By lemma \ref{lemma:maximality} we need only maintain connected components of $F$ since the connected components $C$ with $f(C) = 1$ are precisely the minimal violated cuts. There are a polynomial number of connected components at each iteration (hence a polynomial number of non-zero $y_S$), and since we have a polynomial time oracle for $f$ this implies step $2(a)$ can be done in polynomial time.
\end{proof}
\paragraph{}
Now we need to check the approximation factor. We present the following reduction from class as a lemma:
\begin{lemma}\label{lemma:degree}
Let $w_i$ denote the number of minimally violated sets at iteration $i$.  Let $\cS_i$ denote the set of minimal violated cuts at iteration $i$. If $\sum_{S\in \cS_i} |\delta_F(S)| \leq 2w_i$ for all $i$ then $\cA$ is a $2$-approximation algorithm for $(IP)$ (for any $01$-function $f$).
\end{lemma}
\begin{proof}
It is immediate from our construction of $x$ and $y$ that if $e \in F$ then
$$\sum_{S : e\in\delta(S)} y_S = c(e).$$
Thus we have the objection function value on $F$ is
$$\sum_{e\in F} c(e) = \sum_{e\in F} \sum_{S : e\in\delta(S)} y_S = \sum_{S\subseteq V} |\delta_F(S)|y_S.$$
Let $\Delta_i$ denote the amount by which all minimal violated cuts get increased in iteration $i$. Then
$$\sum_{S\subseteq V} |\delta_F(S)|y_S = \sum_{i} \sum_{S\in \cS_i} |\delta_F(S)|\Delta_i \leq \sum_{i} 2w_i \Delta_i = 2\sum_{S\subseteq V} f(S) y_S.$$
The inequality follows from our hypothesis, and the last equality follows since $f$ is a $01$-function for which non-zero $y_S$ (minimally violated sets) have $f(S) = 1$. 
\end{proof}
\paragraph{}
Now we intend to show that the hypothesis of the previous lemma holds. Consider some iteration $i$ of $\cA$, and let $A$ be the of edges chosen so far at the end of $i$. Let $B = F \cap A$. Construct a graph $H$ by taking the graph $(V,F)$ and contracting the connected components of $(V,B)$ to vertices. Then $E(H) = F\backslash B$. Let $S_v$ denote the set of vertices in $G$ corresponding to vertex $v \in V(H)$. It is not hard to see that $H$ is a forest. Indeed if $H$ has a cycle $C$ then, since for all $v \in V(C)$, $S_v$ is a connected component in $G$, we can find a cycle $C'$ in $G$ by expanding each $v \in V(C)$ and joining its vertices which lie on $C$ by a path in $S_v$. But $F$ is a forest since it is necessary that any edge $e$ be a bridge to survive the reverse deletion step as $f$ is a $01$-function (observe that for all $S$ $|\delta(S) \backslash \{e\}| \geq 1 \geq f(S)$ if $e$ is not a bridge).
\paragraph{}
Now let $W\subseteq V(H)$ be the set of vertices of $H$ such that for all $w \in W$, $f(S_w) = 1$. Then $\{S_w : w\in W\} = \cS_i$. Let $d_v$ denote the degree of vertex $v$ in $H$. So the condition
$$\sum_{S \in \cS_i} |\delta_F(S)| \leq 2w_i$$
is equivalent to
$$\sum_{w \in W} d_w \leq 2|W|.$$
\begin{lemma}\label{lemma:leaves}
$\sum_{w\in W} d_w \leq 2|W|$.
\end{lemma}
\begin{proof}
First we claim that for every connected component $C$ of $H$, at most one leaf of $C$ satisfies $f(S_v) = 0$. Suppose for a contradiction there exists some connected component $C$ of $H$ that has two leaves $v$ and $w$ such that $f(S_v) = f(S_w) = 0$. Let $S_C = \cup_{u\in C} S_u$. Let $e_v, e_w \in F$ be the edges incident to $v$ and $w$ respectively in $H$. By minimality via the reverse deletion step, $F\backslash \{e_v\}$ and $F\backslash \{e_w\}$ are infeasible. But then $f(S_C\backslash S_v) =1 $ and $f(S_C\backslash S_w) = 1$ as these are precisely the cuts which would have no incident edges when $e_v$ and $e_w$ are removed respectively (we already know $f(S_v) = f(S_w) = 0$). Since $C$ is a connected component of $H$ we have $f(S_C) = 0$ by the feasibility of $F$. Hence
$$f(S_v) = f(S_w) = f(S_C) = 0 \quad\text{and}\quad f(S_C\backslash S_v) = f(S_C \backslash S_w) = 1$$
but $S_v, S_w \subseteq S_C$ and $S_v \cap S_w = \emptyset$. This contradiction property (\ref{eq:1}).
\paragraph{}
Now we proceed to demonstrate the inequality. First discard isolated vertices of $H$ since they affect the count in no way. Let $c$ denote the number of connected components of $H$. Now we have,
\begin{align*}
\sum_{w\in W} d_w &= \sum_{v \in H} d_v - \sum_{v\not\in W} d_v \\
&= 2|H| -2c - \sum_{v\not\in W} d_v &\text{since $H$ is a forest} \\
&\leq 2|H| - 2c - 2(|H| - |W| - c) \\
&= 2|W|.
\end{align*}
The inequality follows from our claim which implies all vertices not in $W$ have degree at least $2$ except for one leaf per connected component of $H$, which has degree $1$.
\end{proof}
\paragraph{}
Hence combining lemma \ref{lemma:degree} and lemma \ref{lemma:leaves} with our discussion about $H$ we have shown that $\cA$ returns a solution $F$ with $$c(F) \leq 2\sum_{S\subseteq V} f(S) y_S \leq 2\cdot OPT,$$
where $OPT$ is the optimal solution of $(IP)$. Therefore we have shown that
\begin{theorem}
$\cA$ is a $2$-approximation algorithm for $(IP)$.
\end{theorem}

%Q2
\section{}
\paragraph{}
Let $G = (V,E)$ be a metric graph with edge costs $c: E\rightarrow \R_+$. Consider the problem of finding a minimum cost subset of edges $F\subseteq E$ such that each connected component of $(V,F)$ is an eulerian subgraph with at least $100$ vertices. We will denote this problem $(E100)$.
\paragraph{}
Define the function $f:2^V \rightarrow \{0,2\}$ by:
$$f(S) = \begin{cases}
2, &\text{if } 1\leq |S| < 100 \\
0, &\text{otherwise.}
\end{cases}$$
Then we can define a linear programming relaxation $(LP)$ for $(E100)$:
\begin{align*}
\min\ \sum_{e\in E} c(e) x_e&\\
\text{s.t.}\  \sum_{e\in\delta(S)} x_e &\geq f(S) &\forall S \subseteq V \\
x&\geq 0.
\end{align*}
To justify this relaxation consider a feasible solution $F$ to $(E100)$. Let $x^F$ be the incidence vector of $F$. Then $x^F$ is feasible for $(LP)$. Let $S \subseteq V$ such that $1 \leq |S| < 100$. Since $|S| < 100$, $S$ does not contain any single connected component of $(V,F)$. Since $1\leq |S|$, $S$ has non-trivial intersection with some connected component $C$ of $(V,F)$ (i.e. $S\cap C \neq \emptyset$ and $S\cap C \supset C$). Since $F$ is feasible, $C$ is eulerian. Hence the degree of each vertex in $C$ (with respect to $F$) is even. Thus $|E(C\cap S, C\backslash S)| \geq 2$. Therefore
$$\sum_{e \in \delta(S)} x_e \geq \sum_{e \in E(C\cap S, C\backslash S)} x_e \geq 2 \geq f(S),$$
and hence $x$ is feasible for $(LP)$.
\paragraph{}
We now define another function $g: 2^V \rightarrow \{0,1\}$, given by
$$g(S) = f(S)/2 = \begin{cases}
1, &\text{if } 1\leq |S| < 100 \\
0, &\text{otherwise.}
\end{cases}$$  
The integer program $(IP)$ for $g$ given by:
\begin{align*}
\min\ \sum_{e\in E} c(e) x_e&\\
\text{s.t.}\  \sum_{e\in\delta(S)} x_e &\geq g(S) &\forall S \subseteq V \\
x&\geq 0 \\
x\in \Z^{|E|}
\end{align*}
describes the problem of finding a forest where each connected component is a tree with at least $100$ vertices. From our choice of $g$ it is clear that the connected components of any feasible solution must have at least $100$ vertices, as the cut defined by any smaller component does not satisfy the inequality for $g$. Optimal solutions to $(IP)$ do not have cycles, as we can remove any single edge present in a cycle and preserve connectivity.
\paragraph{}
Now we observe that $g$ satisfies property (\ref{eq:1}) from question $1$. Let $S \subseteq V$ and let $A, B \subseteq S$ such that $A \cap B = \emptyset$. Suppose that $g(S)=g(A) = g(B) = 0$. If $g(S\backslash A) = 1$ then $1 \leq |S\backslash A| < 100$. But $B \subseteq S\backslash A$, and hence $|B| \leq |S\backslash A|$. Since $g(B) = 0$ this means $|B| = 0$, that is $B = \emptyset$. Thus $g(S\backslash B) = g(S) = 0$. Similarly we can show if $g(S\backslash B) = 1$ then $g(S\backslash A) = 0$.  Hence property $(\ref{eq:1})$ is satisfied by $g$.
\paragraph{}
Consider the following algorithm, $\cA$, for $(E100)$:
\begin{enumerate}
\item Set $S = \emptyset$.
\item Solve $(IP)$ for $g$ using the algorithm of question $1$, obtaining a forest $F$.
\item For each connected component $T$ of $(V,F)$:
	\begin{enumerate}
	\item Compute a matching $M$ on the odd vertices of $T$.
	\item Shortcut $T\cup M$ obtaining an Euler Tour $A$.
	\item Set $S = S \cup A$.
	\end{enumerate}
\item Return $S$.
\end{enumerate}
\paragraph{}
We claim that $\cA$ is a $2$-approximation algorithm for $(E100)$. In question $1$ we showed that step $2$ can be done in polynomial time. In the lecture notes we have shown that the shortcutting process of each iteration of step $3$ can be done in polynomial time. Further there are a polynomial number of components the shortcutting is done on, so step $3$ also runs in polynomial time. Therefore $\cA$ returns a solution in polynomial time. The solution is feasible since each component is a cycle (hence is eulerian) on at least $100$ vertices.
\paragraph{}
We now demonstrate the approximation factor holds. We know from lecture that the shortcutting process of step $3$ produces a set of cycles of cost at most twice that of the original trees in a metric graph. That is,
$$c(A)\leq 2c(F).$$
Let $y$ be the optimal dual solution produces by the primal-dual algorithm of step $1$. We have from question $1$ that
$$c(F) \leq 2\sum_{S\subseteq V} g(S) y_S = \sum_{S\subseteq V} f(S) y_S \leq OPT.$$
where $OPT$ is the optimal solution to $(LP)$. This follows since the dual to $(LP)$ (with respect to $(E100)$) has the same feasible region as the dual to the linear programming relaxation of $(IP)$. Combining inequalities we see that
$$c(A) \leq 2c(F) \leq 2 \cdot OPT.$$
Hence $\cA$ is a $2$-approximation algorithm for $(E100)$.$\blacksquare$

%Q3
\section{}
\paragraph{}
Consider again the problem $(IP)$, associated with a graph $G=(V,E)$ with edges costs $c: E \rightarrow \R_+$ given by:
\begin{align*}
\min\ \sum_{e\in E} c(e) x_e&\\
\text{s.t.}\  \sum_{e\in\delta(S)} x_e &\geq f(S) &\forall S \subseteq V \\
x&\geq 0 \\
x&\in \Z^{|E|}.
\end{align*}
Suppose that $f:2^V \rightarrow \N$ is symmetric, meaning for all $S\subseteq V$:
$$f(S) = f(V\backslash S)$$
and that $f$ is subadditive, meaning for all $A,B \subseteq V$:
$$f(A\cup B) \leq f(A) + f(B).$$
We also suppose that the graph $G$ is a cycle on $n$ vertices.
%3a
\subsection*{a}
\paragraph{}
We begin by working towards a compact formulation for $(IP)$ on $G$. Since $G$ is a cycle we may label the vertices of $G$ as $V(G) = \{v_1,\dots, v_n\}$ such that $v_iv_j \in E(G)$ if and only if $$j \equiv i\pm1\mod n$$. For $v_i, v_j \in V(G)$ we define the path from $v_i$ to $v_j$ as 
$$P(i, \ell) := \{v_i, v_{i+1}, \dots, v_{j} \}$$
where addition is taken modulo $n$. We define the family of all paths on $G$ as
$$\cP := \{P(v_i, v_j) : v_i, v_j \in V(G)\}.$$
Observe that the size of $\cP$ is polynomial in $n$, in particular $|\cP| = O(n^2)$ (since there are $n$ choices for $v_i$ and $n$ choices for $v_j$).
\begin{lemma}\label{lemma:compact}
The problem $(IP)$ on $G$ is equivalent to the compact integer program $(\overline{IP})$ given by:
\begin{align*}
\min\ \sum_{e\in E} c(e) x_e&\\
\text{s.t.}\  \sum_{e\in\delta(P)} x_e &\geq f(P) &\forall P \in \cP \\
x&\geq 0 \\
x&\in \Z^{|E|}.
\end{align*}
\end{lemma}
\begin{proof}
Since the objective functions are the same, we need only demonstrate that the feasible regions of $(IP)$ and $(\overline{IP})$ are equal. Since each $P \in \cP$ is a subset of $V$, it is clear that the feasible region of $(IP)$ is a subset of the feasible region of $(\overline{IP})$. So we just have to show the reverse containment.
\paragraph{}
Let $x$ be a feasible solution for $(\overline{IP})$. Let $S \subseteq V$. If $S = V$ then $f(S)= f(V)=0\leq x(\delta(S)) $ (otherwise $(\overline{IP})$ and $(IP)$ are infeasible). By symmetry of $f$ the same holds for $S =\emptyset$. Thus we may assume $\emptyset \subset S \subset V$. If we consider the connected components of $G[S]$ we observe that they are all paths. Hence there exist disjoint $P_1, \dots, P_k \in \cP$ such that
$$S = P_1 \dot\cup \dots \dot\cup P_k$$
where each $P_i$ corresponds to a connected component of $G[S]$. Now we see that
\begin{align*}
f(S) &= f(P_1 \dot\cup \dots \dot \cup P_k) \\
&\leq f(P_1) + \dots + f(P_k) &\text{by subadditivity} \\
&\leq x(\delta(P_1)) + \dots x(\delta(P_k)) &\text{by feasibility of $x$}.
\end{align*}
We claim that the edges incident to each path are disjoint. To see this, suppose that we have $P_i$ and $P_j$ with $\delta(P_i) \cap \delta(P_j) \neq \emptyset$. Let $v_iv_j \in \delta(P_i) \cap \delta(P_j)$. Since $P_i$ and $P_j$ are disjoint we may assume $v_i \in P_i \backslash P_j$ and $v_j \in P_j \backslash P_i$. But then $P_i$ and $P_j$ are connected by $v_iv_j$ violating that they are distinct connected components. So the claim holds. Thus we observe
\begin{align*}
f(S) &\leq x(\delta(P_1)) + \dots x(\delta(P_k)) \\
&= x(\bigcup_{i=1}^k \delta(P_i)) &\text{by disjointness}\\
&= x(\delta(S)) &\text{by disjointness}.
\end{align*}
Therefore $x$ satisfies the cut constraint for all $S$, and hence $x$ is feasible for $(IP)$.
\end{proof}
\paragraph{}
Hence by lemma \ref{lemma:compact} it is equivalent for us to solve $(\overline{IP})$. Now we define a linear programming relaxation for $(\overline{IP})$, which we will denote $(\overline{LP})$:
\begin{align*}
\min\ \sum_{e\in E} c(e) x_e&\\
\text{s.t.}\  \sum_{e\in\delta(P)} x_e &\geq f(P) &\forall P \in \cP \\
x&\geq 0.
\end{align*}
Since $(\overline{LP})$ has a polynomial number of constraints and variables it can be solved in polynomial time. We now define our rounding-based approximation algorithm, $\cA$, for $(\overline{IP})$:
\begin{enumerate}
\item Solve $(\overline{LP})$ on $G$  returning solution $\bar{x}$.
\item Let $\bar{x}_I \in \Z^{|E|}$ denote the integral part of $\bar{x}$. Let $\bar{x}_F \in [0,1)^{|E|}$ denote the fractional part of $\bar{x}$. Thus $\bar{x} = \bar{x}_I + \bar{x}_F.$
\item Construct solution $x \in \Z^{|E|}$ by rounding. For each $e \in E$ set:
$$x_e = \begin{cases}
\bar{x}_I + 1, &\text{if } x_F \geq 0.5 \\
\bar{x}_I, &\text{otherwise.}
\end{cases}$$
\item Return solution $x$.
\end{enumerate}
\begin{lemma}\label{lemma:feasible-rounding}
If $\bar{x}$ is a feasible solution to $(\overline{LP})$ then the $x$ obtained from rounding $\bar{x}$ in step $3$ of $\cA$ is a feasible solution to $(\overline{IP})$.
\end{lemma}
\begin{proof}
It is immediate from the description of step $3$ that $x$ is integral and non-negative. We need only verify the cut constraints. Let $P \in \cP$. Let $v_i, v_j \in P$ be the start and end vertices of $P$. That is,
$$\delta(P) = \{v_{i-1}v_i,  v_jv_{j+1}\}.$$
For convenience let $e_i = v_{i-1}v_i$ and let $e_j = v_jv_{j+1}$. Since $\bar{x}$ is feasible for $(\overline{LP})$:
$$ f(P)\leq \bar{x}(e_i) + \bar{x}(e_j) = \bar{x}_I(e_i) + \bar{x}_F(e_i)  + \bar{x}_I(e_j) + \bar{x}_F(e_j).$$
Since $f$ maps to $\N$ we have:
$$f(P) \leq \floor{\bar{x}_I(e_i) + \bar{x}_F(e_i)  + \bar{x}_I(e_j) + \bar{x}_F(e_j)}.$$
We consider the following cases: Case 1: $\bar{x}_F(e_i) + \bar{x}_F(e_j) < 1$ and Case 2: $\bar{x}_F(e_i) + \bar{x}_F(e_j) \geq 1$.
\paragraph{}
In Case 1 the fractional parts vanish during the floor-ing operation and hence
$$ f(P) \leq \bar{x}_I(e_i) + \bar{x}_I(e_j)  \leq x(e_i) + x(e_j).$$
In Case 2 we observe that $x_F(e_i) + x_F(e_j) = 1 + \alpha$ for some $\alpha \in [0,1)$. Thus
$$f(P) \leq \floor{\bar{x}_I(e_i) + \bar{x}_F(e_i)  + \bar{x}_I(e_j) + \bar{x}_F(e_j)} = \bar{x}_I(e_i) + \bar{x}_I(e_j)  + 1.$$
Since $\bar{x}_F(e_i) + \bar{x}_F(e_j) \geq 1$ at least one of $\bar{x}_F(e_i) ,\bar{x}_F(e_j) $ is greater than or equal to $0.5$. Assume without loss of generality $\bar{x}_F(e_i) \geq 0.5$. Then $x_(e_i) = \bar{x}_I(e_i) + 1$. So
$$f(P) \leq \bar{x}_I(e_i) + \bar{x}_I(e_j)  + 1 = x(e_i) +\bar{x}(e_j) \leq x(e_i) + x(e_j).$$
Therefore in either case $x$ satisfies the cut constraint for $P$. Therefore $x$ is feasible for $(\overline{IP})$.
\end{proof}
\begin{theorem}
The algorithm $\cA$ is a $2$-approximation algorithm for $(\overline{IP})$.
\end{theorem}
\begin{proof}
\paragraph{}
As we have argued above, $(\overline{LP})$ can be solved in polynomial time, so step $1$ can be done in polynomial time. Step $3$ can be done in $O(|E|)$ time. Hence $\cA$ terminates in polynomial time. From lemma \ref{lemma:feasible-rounding} we see that $x$ is feasible for $(\overline{IP})$. It remains to check the approximation factor.
\paragraph{}
Let $e \in E$. If $\bar{x}(e) < 0.5$ then we have immediately $x(e) \leq \bar{x}(e)$ and so trivially we have $x(e) \leq 2\bar{x}(e)$. It remains to consider the pertinent case when $\bar{x}(e) \geq 0.5$. Therein we have
\begin{align*}
\frac{x(e)}{\bar{x}(e)} &= \frac{\bar{x}_I(e) + 1}{\bar{x}_I(e) + \bar{x}_F(e)} \\
&\leq  \frac{\bar{x}_I(e) + 1}{\bar{x}_I(e) +0.5} \\
&= \frac{2\bar{x}_I(e) + 2}{2\bar{x}_I(e) + 1}.
\end{align*}
Since $\frac{2\bar{x}_I(e) + 2}{2\bar{x}_I(e) + 1}$ is a non-increasing function, it is maximized when $\bar{x}_I(e) = 0$. Hence
$$\frac{2\bar{x}_I(e) + 2}{2\bar{x}_I(e) + 1} \leq \frac{2\cdot 0 + 2}{2\cdot 0 + 1} = 2.$$
Thus $x(e) \leq 2\bar{x}(e)$ as desired. Therefore
$$\sum_{e\in E}c(e) x(e) \leq \sum_{e \in E}c(e)2\bar{x}(e) = 2\sum_{e\in E}\bar{x}(e) \leq 2\cdot OPT,$$
where $OPT$ is the cost of an optimal solution to $(\overline{IP})$ (the last inequality follows immediately from $\bar{x}$ being optimal for $(\overline{LP})$. So $\cA$ is a $2$-approximation algorithm.
\end{proof}
%3b
\subsection*{b}
\paragraph{}
We make an observation about optimal solutions to $(\overline{IP})$ which contain a $0$-valued edge. This will lead us to an approximation algorithm with a better bound than the one found in $3(a)$.
\begin{lemma}\label{lemma:0edge}
Let $x$ be an optimal solution $(\bar{IP})$. Suppose there exists $v_iv_{i+1} \in E(G)$ such that $x(v_iv_{i+1}) = 0$. Reindex (by subtracting $i$ from each index) $V(G)$ so that $i = 0$. That is $x(v_0v_1)= 0$. Then for all $v_jv_{j+1} \in E(G)$, $j \neq 0$,
$$x(v_jv_{j+1}) = f(P(v_1, v_j)).$$
\end{lemma}
\begin{proof}
For each $v_jv_{j+1} \neq v_0v_{1} \in E(G)$. Since $x$ is feasible and $x(v_0v_{1}) = 0$,
$$f(P(v_1, v_j)) \leq x(\delta(P(v_1,v_j))) = x(v_jv_{j+1}) + x(v_0v_{1}) = x(v_jv_{j+1}).$$
Suppose for a contradiction there exists $e_j = v_{j}v_{j+1} \in E(G)$ such that
$$f(P(v_1,v_j)) < x(v_jv_{j+1}).$$
Since $f(V) = 0$, $j\neq 0$. Let $x' \in \Z^{|E|}$ we given by
$$x'(e) = \begin{cases}
x(e), &\text{ if $e\neq v_jv_{j+1}$} \\
f(P(v_1,v_{j})), &\text{if $e=v_jv_{j+1}$}. 
\end{cases}$$
We claim that $x'$ is feasible for $(\overline{IP})$. By the feasibility of $x$, we need only check cut constraints involving $v_jv_{j+1}$. Let $v_i \in V(G)$, such that $i \neq j-1$. We consider $P(v_i,v_j)\in \cP$.
\paragraph{}
First consider the case where $0 < i \leq j$. In this case $v0,v_1 \in P(v_{j+1}, v_{i-1})$. We compute:
\begin{align*}
f(P(v_i,v_j)) &= f(P(v_{j+1}, v_{i-1})) &\text{by symmetry} \\
&\leq f(P(v_{j+1}, v_0)) + f(P(v_{1},v_{i-1})) &\text{by subadditivity} \\
&= f(P(v_1, v_j)) + f(P(v_1, v_{i-1})) &\text{by symmetry} \\
&= x'(v_jv_{j+1}) + f(P(v_1, v_{i-1})) &\text{by definition of $x'$} \\
&\leq x'(v_jv_{j+1}) + x(v_1v_{i-1}) +x(v_0v_1) &\text{by feasibility of $x$} \\
&= x'(v_jv_{j+1}) + x'(v_1v_{i-1}) &\text{by definition of $x'$, and $x(v_0v_1)=0$} \\
&=x'(\delta(P(v_i,v_j))).
\end{align*}
Hence $x'$ satisfies the cut constraint.
\paragraph{}
Now consider the case where $i > j$. In this case $v_0,v_1 \in P(v_i,v_j)$. We compute (similarly):
\begin{align*}
f(P(v_i,v_j))&\leq f(P(v_{i}, v_0)) + f(P(v_{1},v_{j})) &\text{by subadditivity} \\
&= f(P(v_1, v_{i-1})) + f(P(v_1, v_{j})) &\text{by symmetry} \\
&= f(P(v_1,v_{i-1})) + x'(v_jv_{j+1}) &\text{by definition of $x'$} \\
&\leq x(v_1v_{i-1}) + x(v_0v_1) + x'(v_jv_{j+1}) &\text{by feasibility of $x$} \\
&= x'(v_jv_{j+1}) + x'(v_1v_{i-1}) &\text{by definition of $x'$, and $x(v_0v_1)=0$} \\
&=x'(\delta(P(v_i,v_j))).
\end{align*}
Therefore in either case $x'$ is feasible. But
\begin{align*}
\sum_{e\in E} c(e) x(e) &= c(v_jv_{j+1})x(v_jv_{j+1}) + \sum_{e\in E\backslash\{v_jv_{j+1}\}} c(e)x(e) \\
&> c(v_jv_{j+1})x'(v_jv_{j+1}) +  \sum_{e\in E\backslash\{v_jv_{j+1}\}} c(e)x(e) &\text{since $x'(v_jv_{j+1}) = f(P(v_1,v_j) < x(v_jv_{j+1})$} \\
&= c(v_jv_{j+1})x'(v_jv_{j+1}) + \sum_{e\in E\backslash\{v_jv_{j+1}\}} c(e)x'(e) &\text{by our choice of $x'$} \\
&= \sum_{e\in E} c(e) x'(e),
\end{align*}
contradicting the optimality of $x$.
\end{proof}
\paragraph{}
This lemma tells us precisely how to construct optimal solutions to $(\overline{IP})$ in the presence of $0$-valued edges. The idea for our algorithm is to construct from this lemma each potential solution with a $0$-valued edge, as well as an approximate solution with no $0$-valued edges, and return the best possible of those $|E|+1$ solutions.
\paragraph{}
In our algorithm we will use the following linear program, which is $(\overline{LP})$ with the constraint $x \geq 1$ instead of $x\geq 0$. We define the linear program $(\tilde{LP})$ as
\begin{align*}
\min\ \sum_{e\in E} c(e) x_e&\\
\text{s.t.}\  \sum_{e\in\delta(P)} x_e &\geq f(P) &\forall P \in \cP \\
x&\geq 1.
\end{align*}
\paragraph{}
Consider the following improved algorithm for solving $(\overline{IP})$ which we will denote by $\cA'$:
\begin{enumerate}
\item Let $E' = \emptyset$. For each $e=v_iv_{i+1} \in E$:
	\begin{enumerate}
	\item Construct the solution $x^{e}$ as follows.
	\item Set $x^{e}(e)=0$.
	\item For each $v_jv_{j+1} \neq e$ Set $x^{e}(v_jv_{j+1}) = f(P(v_{i+1},v_j))$.
	\item If $x^{e}$ is feasible for $(\overline{IP})$ add $e$ to $E'$.
	\end{enumerate} 
\item Solve $(\tilde{LP})$ on $G$  returning solution $\bar{x}$.
\item Let $\bar{x}_I \in \Z^{|E|}$ denote the integral part of $\bar{x}$. Let $\bar{x}_F \in [0,1)^{|E|}$ denote the fractional part of $\bar{x}$. Thus $\bar{x} = \bar{x}_I + \bar{x}_F.$
\item Construct solution $x \in \Z^{|E|}$ by rounding. For each $e \in E$ set:
$$x_e = \begin{cases}
\bar{x}_I + 1, &\text{if } x_F \geq 0.5 \\
\bar{x}_I, &\text{otherwise.}
\end{cases}$$
\item Return $\arg\min\{c^Tx : x \in \{x^e : e \in E'\} \cup \{x\}\}$.
\end{enumerate}
\begin{theorem}
The algorithm $\cA'$ is a $\frac{4}{3}$-approximation algorithm for $(\overline{IP})$.
\end{theorem}
\begin{proof}
Step $1$ can be done in $O(|E|)$ iterations, each using $O(|E|)$ calls to the (we may assume polynomial time) oracle for $f$, and hence runs in polynomial time. Step $2$ can be done in polynomial time via the same argument used for $(\overline{LP})$. Step $4$ can be done in $O(|E|)$ time, as can step $5$. Therefore $\cA'$ runs in polynomial time. 
\paragraph{}
By definition, for each $e \in E'$, $x^{e}$ is feasible. Since the feasible region of $(\tilde{LP})$ is a subset of the feasible region of $(\overline{LP})$, $\bar{x}$ is feasible for $(\overline{LP})$ and therefore by lemma \ref{lemma:feasible-rounding} $x$ is feasible for $(\overline{IP})$ as Step $4$ of $\cA'$ is the same rounding procedure as step $3$ of $\cA$. Hence $\cA'$ returns a feasible solution for $(\overline{IP})$.
\paragraph{}
If there exists an optimal solution $x^*$ for $(\overline{IP})$ and $e \in E$ such that $x^*(e) = 0$ then $x^*(e) = x^{e}$ by lemma \ref{lemma:0edge} and $e \in E'$. Thus in step $5$ $\cA'$ returns a solution of cost at most that of $x^{e} = x^*$ which is therefore optimal. Trivially such a solution is of cost at most $\frac{4}{3}$ that of optimal.
\paragraph{}
It remains to consider the case where no optimal solution $x^*$ for $(\overline{IP})$ has $e\in E$ such that $x^*(e) = 0$. Since $x^*$ is integral and no component is zero optimal solutions for $(\overline{IP})$ satisfy the constraint $x\geq 1$. Therefore $(\tilde{LP})$ is a relaxation of $(\overline{IP})$.
\paragraph{}
Now it suffices to show that the returned solution $x$ has cost at most $\frac{4}{3}$ that of $\bar{x}$. Let $e \in E$. If $\bar{x}(e) < 0.5$ then we have immediately $x(e) \leq \bar{x}(e)$ and so trivially we have $x(e) \leq \frac{4}{3}\bar{x}(e)$. It remains to consider the pertinent case when $\bar{x}(e) \geq 0.5$. Therein we have
\begin{align*}
\frac{x(e)}{\bar{x}(e)} &= \frac{\bar{x}_I(e) + 1}{\bar{x}_I(e) + \bar{x}_F(e)} \\
&\leq  \frac{\bar{x}_I(e) + 1}{\bar{x}_I(e) +0.5} \\
&= \frac{2\bar{x}_I(e) + 2}{2\bar{x}_I(e) + 1}.
\end{align*}
Since $\frac{2\bar{x}_I(e) + 2}{2\bar{x}_I(e) + 1}$ is a non-increasing function, it is maximized when $\bar{x}_I(e) = 1$ (since $\bar{x}_I(e) \geq 1$). Hence
$$\frac{2\bar{x}_I(e) + 2}{2\bar{x}_I(e) + 1} \leq \frac{2\cdot 1 + 2}{2\cdot 1 + 1} = \frac{4}{3}.$$
Thus $x(e) \leq \frac{4}{3}\bar{x}(e)$ as desired. 
\end{proof}

%Q4
\section{}
\paragraph{}
Consider the packing problem $(P)$: Let $U$ be a set of elements. Let $\cS$ be a non-empty family of sets $S\subseteq U$. The goal is to find a subcollection of $\cS$ of maximum cardinality, subject to the constraint that each element is contained in at most $1$ set.
\paragraph{}
We can write a linear programming relaxation for $(P)$, denoted $(LP)$ as
\begin{align*}
\max \sum_{S\in\cS} x_S\ & \\
\text{s.t.} \sum_{S:e\in S} x_S &\leq 1 &\forall e\in U \\
x&\geq 0.
\end{align*}
Let $k = \max_{S\in \cS} \{ |S|\}$.
%pa
\subsection*{a}

\end{document}

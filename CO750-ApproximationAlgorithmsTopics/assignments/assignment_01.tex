\documentclass[letterpaper,12pt,oneside,onecolumn]{article}
\usepackage[margin=1in, bottom=1in, top=1in]{geometry} %1 inch margins
\usepackage{amsmath, amssymb, amstext}
\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{theorem}
\usepackage{tikz}
\usepackage{tkz-berge}

%Macros
\newcommand{\A}{\mathbb{A}} \newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}} \newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}} \newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
 
 
\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\newenvironment{proof}{{\bf Proof:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofof}[1]{{\bf Proof of #1:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofofnobox}[1]{{\bf#1:  }}{}\newenvironment{example}{{\bf Example:  }}{\hfill\rule{2mm}{2mm}}

%\renewcommand{\thesection}{\lecnum.\arabic{section}}
%\renewcommand{\theequation}{\thesection.\arabic{equation}}
%\renewcommand{\thefigure}{\thesection.\arabic{figure}}

\newtheorem{fact}{Fact}[section]
\newtheorem{lemma}[fact]{Lemma}
\newtheorem{theorem}[fact]{Theorem}
\newtheorem{definition}[fact]{Definition}
\newtheorem{corollary}[fact]{Corollary}
\newtheorem{proposition}[fact]{Proposition}
\newtheorem{claim}[fact]{Claim}
\newtheorem{exercise}[fact]{Exercise}
\newtheorem{note}[fact]{Note}
\newtheorem{conjecture}[fact]{Conjecture}

\newcommand{\size}[1]{\ensuremath{\left|#1\right|}}
\newcommand{\ceil}[1]{\ensuremath{\left\lceil#1\right\rceil}}
\newcommand{\floor}[1]{\ensuremath{\left\lfloor#1\right\rfloor}}

%END MACROS
%Page style
\pagestyle{fancy}

\listfiles

\raggedbottom

\lhead{2017-02-09}
\rhead{William Justin Toth CO750-Approximation Algorithms Assignment 1} %CHANGE n to ASSIGNMENT NUMBER ijk TO COURSE CODE
\renewcommand{\headrulewidth}{1pt} %heading underlined
%\renewcommand{\baselinestretch}{1.2} % 1.2 line spacing for legibility (optional)

\begin{document}
%Q1
\section{}
\paragraph{}
Let $E$ be the edge set of a graph $G$. Let $\cS$ be the collection of all sets $S \subseteq E$ that are matchings of $G$. Let $w: E \rightarrow \R_+$ be a weight function on the edges. For a subset $\bar{E} \subseteq E$ we denote by $G[\bar{E}]$ the subgraph of $G$ induced by the edges in $\bar{E}$.
	%Pa
\subsection{a}
\paragraph{}
Let $\bar{E} \subseteq E$. Suppose that $M \subseteq \bar{E}$ is an inclusion-wise maximal matching of $G[\bar{E}]$. Let $\bar{M}$ be a matching of $G[\bar{E}]$. Let $f : M \rightarrow \{ \{e_1,e_2\} | e_1, e_2 \in \bar{M}\}$ be defined by
$$ f(uv) = \{e_1, e_2\}$$
where $e_1 \in \bar{M}$ covers $u$ and $e_2 \in \bar{M}$ covers $v$, for each $uv \in M$.
\paragraph{}
We claim that for all $uv \in \bar{M}$, there exists $P \in Im(f)$ (where $Im(f)$ denotes the image of $f$) such that $uv \in P$. To see this suppose for a contradiction that there exists some $uv \in \bar{M}$ where each $P \in Im(f)$ does not contain $uv$. Then by the definition of $f$, neither $u$ nor $v$ are covered by $M$. But this violates that $M$ is inclusion-wise maximal since $uv \not\in M$ and $M \cup \{uv\}$ is a matching. Hence the claim holds.
\paragraph{}
From the previous claim we observe $|\bar{M}| \leq | \cup_{P \in Im(f)} P|$. But this yields a string of inequalities which complete the proof:
\begin{align*}
|\bar{M}| &\leq |\cup_{P \in Im(f)} P| \\
&\leq \sum_{P \in Im(f)} |P| \\
&\leq \sum_{P \in Im(f)} 2 \\
&\leq 2|Im(f)| \\
&\leq 2|M|. 
\end{align*}
With the last inequality following since $M$ is the domain of $f$. Hence we have $|M| \geq \frac{1}{2}|\bar{M}|$ as desired. $\blacksquare$
	%Pb
\subsection{b}
(This argument follows the exact same lines as the argument for problem $2$)
\paragraph{}
We can decided in polynomial time if $M\cup \{e\}$ is a matching in $\cS$ by iterating over $V$, counting the number of edges in $M\cup\{e\}$ which cover each vertex in $V$, and returning ``No" is the count is greater than $1$ for any vertex and ``Yes" otherwise. Thus the algorithm clearly runs in polynomial time and returns a feasible set.
\paragraph{}
Let $S$ be the set of edges returned by the greedy algorithm. Order the edges as $S = \{e_1, \dots, e_n\}$ in the order they were taken during the operation of $\cA$. Let $F_i := E\backslash \tilde{E}$ be the edges removed from $E$ at the edge of iteration $i$ of $\cA$ (the iteration where $e_i$ is added to $S$). Let $S^*$ be the optimal solution and partition $S^* = S^*_1 \cup \dots \cup S^*_n$ where $S^*_i = S^* \cap F_i$.
\paragraph{}
We want that $|S^*_i| \leq 2$ to hold, but we may need to redistribute some edges to ensure that this happens (and we will do this without effecting our ability to bound the optimal solution). Run the following algorithm, denoted $\cB$, to redistribute the edges in our partition of $S^*$:
\begin{enumerate}
\item For $i = 1, \dots, n$
\begin{enumerate}
\item While $|S^*_i| > 2$: take an edge $e$ from $S^*_i$ and redistribute $e$ to some $S^*_j$ for $j<i$ with $|S^*_j| < 2$.
\end{enumerate}
\end{enumerate}
\paragraph{Claim 1}
After the operation of $\cB$, for all $i = 1, \dots, n$, $|S^*_i| \leq 2$ and $w_{e_i} \geq w_e$ for all $e \in S^*_i$.
\paragraph{Proof of Claim 1}
We proceed by induction. For $e_1$ $\{e_1\}$ is inclusion-wise maximal on $\cS[F_1]$ (by definition of $F_1$), and $S_1^*$ is feasible on $\cS[F_1]$ since $S^*_1 \subseteq F_1$ and $S^* \in \cS$. Thus we have $|\{e_1\}| \geq \frac{1}{2} |S_1^*|$, and hence
$$|S^*_1| \leq \alpha.$$
Further by our greedy choice of $e_1$, $w_{e_1} \geq w_e$ for all $e \in F_1$ (and hence for all $e \in S^*_1$).
\paragraph{}
Suppose that the claim holds for $S^*_1 ,\dots, S^*_i$. Now by our greedy choice, and induction, $w_{e_{i+1}} \geq w_{e_j} \geq w_e$ for all $e \in S^*_j$ for all $j = 1,\dots, i$. So when $\cB$ redistributes edges of $S^*_{i+1}$ the cost property on the earlier sets is not violated. Also by our greedy choice, any edges that were not moved from $S^*_{i+1}$ by $\cB$ have cost at most $w_{e_{i+1}}$. To finish the proof of the claim, we demonstrate that while $|S_{i+1}^*| > \alpha$ there exists some $S^*_j$ with $j\leq i$ and $|S^*_j| < 2$. Observe that $\{e_1, \dots, e_{i+1}\}$ is inclusion-wise maximal on $\cS[F_1 \cup \dots \cup F_{i+1}]$ by definition of $F_1 \cup \dots \cup F_{i+1}$. Also $\bigcup_{j=1}^{i+1} S^*_j \in \cS[F_1 \cup \dots \cup F_{i+1}]$ so we have that
$$|\{e_1, \dots, e_{i+1}\}| \geq \frac{1}{2} |\bigcup_{j=1}^{i+1} S^*_j | = \frac{1}{2} \sum_{j=1}^{i+1} |S^*_j|.$$
Thus we see that
$$ \frac{1}{i+1}\sum_{j=1}^{i+1} |S^*_j| \leq 2.$$
Now since the desired bound holds on average, if $|S^*_{i+1}| > 2$ there exists some $S^*_j$ with $j \leq i$ and $|S^*_j| < 2$ (otherwise the bound does not hold). Thus we have proven Claim $1$. $\blacksquare$
\paragraph{}
With Claim $1$ in hand we are prepared to compute the desired approximation factor:
\begin{align*}
w(S^*) &= \sum_{i=1}^n c(S^*_i) \\
&\leq \sum_{i=1}^n w_{e_i} |S^*_i| \\
&\leq \sum_{i=1}^n w_{e_i} 2 \\
&= 2 w(S).
\end{align*}
Thus $w(S) \geq \frac{1}{2} w(S^*)$ and hence the approximation factor holds as desired. $\blacksquare$
	%Pc
	\subsection{c}
	\paragraph{}
	Suppose for a contradiction that the Greedy Algorithm is in fact a $(\frac{1}{2} + \epsilon)$-approximation algorithm for some $\epsilon \in (0, \frac{1}{2}]$. Then consider the following example. Let $G$ be the path graph with vertex set $V(G) = \{a,b,c,d\}$ and edge set $E(G) = \{ab, bc,cd\}$. Define the weight function $w : E(G) \rightarrow \R_+$ by
	$$w_{ab} = w_{cd} = 1 \quad\text{and}\quad w_{bc} = 1+\epsilon.$$
	Then the Greedy algorithm would choose the edge $bc$ as its solution. This solution has cost $1+\epsilon$. The optimal solution is to take edges $ab$ and $cd$ with cost $2$. The ratio of these solutions is
	$$ \frac{1+\epsilon}{2} = \frac{1}{2} + \frac{\epsilon}{2} < \frac{1}{2} + \epsilon$$
	Contradicting that the Greedy Algorithm is a $(\frac{1}{2} + \epsilon)$-approximation algorithm. $\blacksquare$
%Q2
\section{}
\paragraph{}
Let $E$ be a set of $m$ elements and $\cS$ a collection of sets $S \subseteq E$. Let $w: E\rightarrow \R_+$ be a weight function. Let $\cS[\bar{E}] := \{ S \in \cS : S \subseteq \bar{E}\}$. Suppose that $\cS$ satisfies the following for a given $\alpha \geq 1$:
\begin{enumerate}
\item $S \in \cS$ implies $S' \in \cS$ for any $S' \subseteq S$
\item For all $\bar{E} \subseteq E$, if $S$ is an inclusion-wise maximal set in $\cS[\bar{E}]$ then $|S| \geq \frac{1}{\alpha} |\bar{S}|$ for all $\bar{S} \in \cS[\bar{E}]$.
\end{enumerate}
We will show the Greedy Algorithm, denoted $\cA$, is a $\frac{1}{\alpha}$-approximation algorithm for the problem of finding a maximum weight $S \in \cS$.
\paragraph{}
Since we have a polynomial time oracle for membership in $\cS$ the algorithm clearly runs in polynomial time and returns a feasible set.
\paragraph{}
Let $S$ be the set of edges returned by the greedy algorithm. Order the edges as $S = \{e_1, \dots, e_n\}$ in the order they were taken during the operation of $\cA$. Let $F_i := E\backslash \tilde{E}$ be the edges removed from $E$ at the edge of iteration $i$ of $\cA$ (the iteration where $e_i$ is added to $S$). Let $S^*$ be the optimal solution and partition $S^* = S^*_1 \cup \dots \cup S^*_n$ where $S^*_i = S^* \cap F_i$.
\paragraph{}
We want that $|S^*_i| \leq \alpha$ to hold, but we may need to redistribute some edges to ensure that this happens (and we will do this without effecting our ability to bound the optimal solution). Run the following algorithm, denoted $\cB$, to redistribute the edges in our partition of $S^*$:
\begin{enumerate}
\item For $i = 1, \dots, n$
\begin{enumerate}
\item While $|S^*_i| > \alpha$: take an edge $e$ from $S^*_i$ and redistribute $e$ to some $S^*_j$ for $j<i$ with $|S^*_j| < \alpha$.
\end{enumerate}
\end{enumerate}
\paragraph{Claim 1}
After the operation of $\cB$, for all $i = 1, \dots, n$, $|S^*_i| \leq \alpha$ and $w_{e_i} \geq w_e$ for all $e \in S^*_i$.
\paragraph{Proof of Claim 1}
We proceed by induction. For $e_1$ $\{e_1\}$ is inclusion-wise maximal on $\cS[F_1]$ (by definition of $F_1$), and $S_1^*$ is feasible on $\cS[F_1]$ since $S^*_1 \subseteq F_1$ and $S^* \in \cS$. Thus we have $|\{e_1\}| \geq \frac{1}{\alpha} |S_1^*|$, and hence
$$|S^*_1| \leq \alpha.$$
Further by our greedy choice of $e_1$, $w_{e_1} \geq w_e$ for all $e \in F_1$ (and hence for all $e \in S^*_1$).
\paragraph{}
Suppose that the claim holds for $S^*_1 ,\dots, S^*_i$. Now by our greedy choice, and induction, $w_{e_{i+1}} \geq w_{e_j} \geq w_e$ for all $e \in S^*_j$ for all $j = 1,\dots, i$. So when $\cB$ redistributes edges of $S^*_{i+1}$ the cost property on the earlier sets is not violated. Also by our greedy choice, any edges that were not moved from $S^*_{i+1}$ by $\cB$ have cost at most $w_{e_{i+1}}$. To finish the proof of the claim, we demonstrate that while $|S_{i+1}^*| > \alpha$ there exists some $S^*_j$ with $j\leq i$ and $|S^*_j| < \alpha$. Observe that $\{e_1, \dots, e_{i+1}\}$ is inclusion-wise maximal on $\cS[F_1 \cup \dots \cup F_{i+1}]$ by definition of $F_1 \cup \dots \cup F_{i+1}$. Also $\bigcup_{j=1}^{i+1} S^*_j \in \cS[F_1 \cup \dots \cup F_{i+1}]$ so we have that
$$|\{e_1, \dots, e_{i+1}\}| \geq \frac{1}{\alpha} |\bigcup_{j=1}^{i+1} S^*_j | = \frac{1}{\alpha} \sum_{j=1}^{i+1} |S^*_j|.$$
Thus we see that
$$ \frac{1}{i+1}\sum_{j=1}^{i+1} |S^*_j| \leq \alpha.$$
Now since the desired bound holds on average, if $|S^*_{i+1}| > \alpha$ there exists some $S^*_j$ with $j \leq i$ and $|S^*_j| < \alpha$ (otherwise the bound does not hold). Thus we have proven Claim $1$. $\blacksquare$
\paragraph{}
With Claim $1$ in hand we are prepared to compute the desired approximation factor:
\begin{align*}
w(S^*) &= \sum_{i=1}^n c(S^*_i) \\
&\leq \sum_{i=1}^n w_{e_i} |S^*_i| \\
&\leq \sum_{i=1}^n w_{e_i} \alpha \\
&= \alpha w(S).
\end{align*}
Thus $w(S) \geq \frac{1}{\alpha} w(S^*)$ and hence the approximation factor holds as desired. $\blacksquare$
%Q3
\section{}
\paragraph{}
Let $G = (V,E)$ be a graph. For any $S, T \subseteq V$ we denote by $E(S,T)$ the set of edges between $S$ and $T$. Formally $E(S,T) = \{\{s,t\} \in E: s \in S, t \in T\}$.
%Pa
\subsection{a}
\paragraph{}
The fact that the Greedy Algorithm runs in polynomial time is obvious. $|E(v,S)|$ can be computed in $O(|E|)$ time and the while loop runs for $O(|V|)$ iterations. Further it clearly returns a cut. It remains to verify the approximation factor of $\frac{1}{2}$ holds.
\paragraph{}
We show that the approximation factor is maintained throughout operation of the algorithm on the graph $G[V\backslash \bar{V}]$ induced by vertices considered so far. After the first iteration only one vertex has been considered and the approximation factor holds trivially on the graph induced by that one vertex. Now for induction let $V' = V \backslash \bar{V}$ be the set of vertices considered so far by the Greedy Algorithm, and at the start of the next iteration the vertex $v \in V$ is being considered. Let $S \subseteq V$ be the cut set considered so far, and let $S^*$ be the optimal cut set on $G[V']$. Let $\bar{S} = V'\backslash S$ and $\bar{S^*} = V' \backslash S^*$. We may assume without loss of generality that the Greedy Algorithm puts $v \in S$ and the optimal solution puts $v \in S^*$ after this iteration (if this does not hold simply relabel $S$ with $\bar{S}$ below, or $S^*$ with $\bar{S^*}$ with respect to which assumption does not hold). 
\paragraph{}
So after this iteration the optimal solution has value
$$|E(S^* \cup \{v\}, \bar{S^*})| =  \sum_{u \in S^* \cup \{v\}} |E(u, \bar{S^*})| = |E(v,\bar{S^*})| + \sum_{u \in S^*} |E(u, \bar{S^*})| \leq |E(v,\bar{S^*})| + 2\sum_{u \in S} |E(u, \bar{S})|$$
with the inequality following by induction. Hence the approximation factor will hold as desired provided that
$$|E(v,\bar{S^*})| \leq 2|E(v,\bar{S})|.$$
Suppose for a contradiction that
$$|E(v,\bar{S^*})| > 2|E(v,\bar{S})|.$$
We observe that
$$|E(v, \bar{S^*})| = |E(v,\bar{S^*}\backslash S)| + |E(v, \bar{S^*}\cap S)| \leq  |E(v, \bar{S})| + |E(v, S)|.$$
The inequality follows since $\bar{S^*}\backslash S \subseteq V' \backslash S = \bar{S}$, and $\bar{S^*} \cap S \subseteq S$. Combining this inequality with the contradiction assumption we see
$$2|E(v,\bar{S})| <|E(v, \bar{S})| + |E(v, S)|$$
and subtracting $|E(v, \bar{S})|$ from both sides yields
$$ |E(v,\bar{S})| < |E(v,S)|.$$
This contradicts our Greedy choice $v \in S$ since such choice implies
$$ |E(v,\bar{S})| \geq |E(v,S)|.$$
Hence we have
$$|E(v,\bar{S^*})| \leq 2|E(v,\bar{S})|.$$
Thus after this iteration the optimal solution has value:
$$|E(S^* \cup \{v\}, \bar{S^*})|  \leq |E(v,\bar{S^*})| + 2\sum_{u \in S} |E(u, \bar{S})| \leq 2|E(v,\bar{S})| + 2\sum_{u \in S} |E(u, \bar{S})| = 2 |E(S \cup \{v\}, \bar{S})|.$$
Therefore $|E(S \cup \{v\}, \bar{S})| \geq \frac{1}{2} |E(S^* \cup \{v\}, \bar{S^*})|$ as desired. 
\paragraph{}
Now we observe that, from the invariant we just demonstrated, upon termination of the algorithm the approximation factor holds for the greedy solution versus the optimal solution on $G[V \backslash \emptyset] = G$, and hence the Greedy Algorithm is $\frac{1}{2}$-approximation algorithm.$\blacksquare$
%Pb
\subsection{b}
\paragraph{}
Our goal is to find $k$ cuts of $G$ $V_1, \dots, V_k \subseteq V$ maximizing:
$$|\bigcup_{i=1}^k \delta(V_i)|.$$
Let $\cB$ denote the $2$-approximation algorithm for max cut given in problem $3a$. Consider the following algorithm, which we will denote by $\cA$:
\begin{enumerate}
\item Set $V_1 = \dots = V_k = \emptyset$. Set $G_1 = G$
\item For $i = 1, \dots, k$
\begin{enumerate}
\item Let $S$ be the cut returned by $\cB$ run on $G_i$.
\item Set $V_i = S$. Set $G_{i+1} = G_i \backslash \delta(S).$
\end{enumerate}
\item Return $V_1, \dots, V_k$.
\end{enumerate}
The idea behind the operation of $\cA$ is to find an optimal cut, then remove the edges of the cut and iterate.
\begin{lemma}\label{lemma:3b1}
Let $\cO = \{V^*_1, \dots, V^*_k\}$ denote the optimal solution. For any $i=1,\dots, k$ let $\cS^i = \{V_1, \dots, V_{i}\}$ be the set of partitions chosen so far at the end of iteration $i$ by $\cA$. Then 
$$2k(c(\cS^i) - c(\cS^{i-1})) \geq c(\cO) - c(\cS^{i-1}).$$
With $S^0 = \emptyset$.
\end{lemma}
\begin{proof}
For $i = 1,\dots, k$ let $S_i^* = V_i^k \backslash \bigcup_{S\in \cS^{i-1}} S$. Then
$$c(\cO) - c(\cS^{i-1}) = c(\{S^*_1,\dots, S^*_l\}) = |\bigcup_{i=1}^k \delta(S^*_i)|.$$
Let $$S^* = \text{arg max}_{S^*_i : i=1,\dots, k} |\delta(S^*_i)|.$$
Then 
$$c(\cO) - c(\cS^{i-1}) \leq k|\delta(S^*)|.$$
Now observe that for all $i = 1,\dots, k$, $\delta(S^*_i) \subseteq E(G_i)$, hence in particular $\delta(S^*) \subseteq E(G_i)$. Thus $|\delta(S^*)|$ has size at most that of an optimal cut in $G_i$. Now by our choice of $V_i$, using $2$-approximation algorithm $\cB$, $2|\delta(V_i)|$ is at the size of an optimal cut in $G_i$. Hence we have
$$|\delta(S^*)| \leq 2|\delta(V_i)|.$$
Thus, combining inequalities, we observe
$$c(\cO) - c(\cS^{i-1}) \leq 2k |\delta(V_i)|.$$
But $|\delta(V_i)| = c(\cS^i) - c(\cS^{i-1})$ and thus
$$c(\cO) - c(\cS^{i-1}) \leq 2k (c(\cS^i) - c(\cS^{i-1}))$$
as desired.
\end{proof}
\paragraph{}
Using the previous lemma, we can demonstrate that $\cA$ is a $\alpha := 1-(1-\frac{1}{2k})^k$-approximation algorithm for the max $k$-cut problem.
\paragraph{Main Proof}
Since $\cB$ can be run in polynomial time, and $k$ is at most $|V|$, it is clear that step $2$ of $\cA$, and hence $\cA$, runs in polynomial time. At termination the algorithm returns $k$ subsets of $V$, so the algorithm returns a feasible solution.
\paragraph{} 
Now to see the approximation factor holds let $\cS^i$ denote the solution maintained by $\cA$ at the end of iteration $i$. Then the returned solution is $\cS^{k}$. Let $\cO$ be an optimal solution. Observe from Lemma \ref{lemma:3b1} that $c(\cS^{i}) \geq \frac{1}{2k} c(\cO) + (1- \frac{1}{2k})c(S^{i-1})$. We will apply this inequality inductively to achieve the bound:
\begin{align*}
c(\cS^{k}) &\geq \frac{1}{2k} c(\cO) + (1- \frac{1}{2k})c(S^{k-1}) \\
		&\geq \frac{1}{2k} c(\cO) + (1- \frac{1}{2k})(\frac{1}{2k} c(\cO) + (1- \frac{1}{2k})c(S^{k-2})) \\
		&\geq \frac{1}{2k} c(\cO) + (1- \frac{1}{2k})(1 + (1-\frac{1}{2k}) + (1-\frac{1}{2k})^2 + \dots + (1-\frac{1}{2k})^{k-1}) \\
		&= \frac{c(\cO)}{2k} \cdot \frac{1-(1-\frac{1}{2k})^k}{1-(1-\frac{1}{2k})} \\
		&=\frac{c(\cO)}{2k} \cdot \frac{1-(1-\frac{1}{2k})^k}{\frac{1}{2k}}\\
		&= (1-(1-\frac{1}{2k})^k)\cdot c(\cO).
\end{align*}
Thus the approximation factor holds as desired. $\blacksquare$
%Q4
\section{}
\paragraph{}
Let $G = (V,E)$ be a graph with edge costs $c: E \rightarrow \R_+$. Let $r \in V$ be a root vertex. Let $R \subseteq V \backslash \{r\}$ be a set of terminals. Let $d_v \in \{2^0, 2^1, \dots, 2^\delta\}$ denote the demand of $v$ for each $v \in R$. We assume we have an $O(1)$-approximation algorithm for the Steiner Tree problem. Then each iteration of step $2$ of the given algorithm runs in polynomial time using the assumed approximation algorithm. There are $\delta+1$ iterations of step $2$ which is $O(\log \max_v d_v)$ and hence polynomial in the number of bits needed to specify the input to the problem. Thus the entirety of step $2$ runs in polynomial time and hence the given algorithm runs in polynomial time. It is clear the tree returned spans $R \cup \{r\}$ since the partial solution at each iteration $i$ spans $R_\delta \cup \dots \cup R_i \cup \{r\}$. Further at each iteration $i$ the paths in the final solution connecting $v \in R_i$ to $r$ are assigned costs $2^i$ on their edges and hence a feasible flow of value $d_v$ from $r$ to $v$ is possible. Thus the algorithm returns a feasible solution in polynomial time.
\paragraph{}
It remains to verify the approximation factor of $O(1)$. We do so by induction on the number of iterations the algorithm needs to run to return a solution. In the base case, that is when $\delta = 0$, the algorithm returns an $O(1)$-approximate Steiner Tree connecting $R \cup \{r\}$ and installs capacity of $1$ on each edge. By observing that the optimal solution in this case is a Steiner Tree connecting $R \cup \{r\}$ with capacity $1$ on each edge we see that the approximation factor holds in this case.
\paragraph{}
Suppose that $\delta > 0$ and for any problem instance with maximum demand $2^{\alpha}$ with $0 \leq \alpha < \delta$ the algorithm returns an $O(1)$ approximate solution. We will describe solutions by an ordered pair $(T, x)$ where $T$ is a tree and $x : T \rightarrow \R_+$ maps edges of $T$ with their corresponding capacity. Then the objective function, which we denote by $f$, in our problem maps feasible solutions $(T,x)$ as follows:
$$f(T,x) = \sum_{e \in T} c_e x_e.$$
\paragraph{}
Let $(T,x)$ be the solution returned by running our algorithm, and let $(T^*, x^*)$ be the optimal solution. Let $T_{\delta}$ be the $O(1)$-approximate Steiner Tree connecting $R_\delta \cup \{r\}$ in the first iteration (iteration $\delta$) of our algorithm. Let $(\bar{T}, x)$ denote the solution returned by our algorithm running on the residual problem with input graph $G_{\delta-1}$ (the resulting graph after contracting $T_{\delta}$ into $r$). Then $T = T_\delta \cup \bar{T}$ and 
$$f(T,x) = f(T_\delta, x) + f(\bar{T},x).$$
If we let $(\bar{T}^*, \bar{x}^*)$ denote the optimal solution for the residual problem with graph $G_{\delta - 1}$ then by our inductive hypothesis
$$f(T,x) \leq f(T_\delta,x) + O(1)f(\bar{T}^*, \bar{x}^*).$$
Now we observe that $T^*$ contains a Steiner Tree $T'$ with terminals $R_\delta \cup \{r\}$. Since $T_\delta$ is an $O(1)$-approximate optimal Steiner tree with terminals $R_\delta \cup \{r\}$ we have $c(T_\delta) \leq O(1) c(T')$. Since $T'$ necessarily installs capacities $2^\delta$ on all of its edges, $x_e^* = 2^\delta$ for all $e \in T'$. To see that a capacity of $2^\delta$ is necessary, suppose some edge $e$ in $T'$ has $x^*_e < 2^\delta$. Since $T'$ is a Steiner Tree, some terminal $v \in R_\delta$ uses $e$ on its unique path to $r$. But then an $rv$-flow along this path has value at most $x^*_e < 2^\delta$ contradicting feasibility of $(T^*, x^*)$. Thus $x_e^* = 2^\delta$ for all $e \in T'$. Hence we have
$$f(T_\delta, x) = \sum_{e \in T_\delta} c_e x_e = 2^\delta c(T_\delta) \leq O(1)2^\delta c(T') = O(1)f(T', x^*) \leq O(1) f(T^*, x^*).$$
Thus we see that
$$f(T,x) \leq O(1) f(T^*,x^*) + O(1) f(\bar{T}^*, \bar{x}^*).$$
Now consider the graph that results from contracting all edges of $T_\delta \cap T^*$ into $r$ in the tree $T^*$. If any cycles form then do the following: while there still exists a cycle $C$ take the edge $e = \text{argmin}_{e\in C} x^*_e$ and remove it. Call the resulting graph $S^*$. 
\paragraph{}
We claim that $(S^*, x^*)$ is feasible for the residual problem on $G_{\delta-1}$. Indeed $S^*$ is a tree since we trimmed one edge from all cycles. Since trimming cycles preserves connectivity, and we contracted edges into $r$, all $rv$-paths in $T^*$ are preserved in $S^*$. Hence for all $v \in R\backslash R_\delta$ there exists an $rv$-path in $S^*$. Now we claim that the capacities on any $rv$-path still admit feasible flows after converting from $T^*$ to $S^*$. Observe that contraction will not decrease the minimum capacity. Further observe that deleted edges had minimum capacity on a cycle, so any flow that used a deleted edge $uv$ could send the same flow through the $uv$-path remaining from the cycle $uv$ was deleted from. Hence the flows on $S^*$ under capacities $x^*$ can still meet all demands in $R\backslash R_\delta$. Therefore $(S^*,x^*)$ (with $x^*$ restricted to $E(G_{\delta-1})$) is feasible for $G_{\delta -1}$. Hence, since $(\bar{T}^*, \bar{x}^*)$ is optimal for $G_{\delta -1}$:
$$f(\bar{T}^*, \bar{x}^*) \leq f(S^*, x^*) \leq f(T^*, x^*).$$
Therefore
\begin{align*}
f(T,x) &\leq O(1)f(T^*,x^*) + O(1)f(\bar{T}^*,\bar{x}^*) \\
&\leq O(1)f(T^*,x^*) + O(1)f(T^*,x^*) \\
&= 2O(1)f(T^*,x^*)\\
&= O(1)f(T^*,x^*).
\end{align*}
So the approximation factor $O(1)$ holds in the inductive case. Thus the algorithm given is a $O(1)$-approximation algorithm.$\blacksquare$

%Q5
\section{}
\paragraph{}
Let $G=(V,E)$ be a metric graph with edge lengths $d: E \rightarrow \N_+$ and let $f : 2^V \rightarrow \N_+$  be a a function satisfying:
\begin{enumerate}
\item (Symmetric) $f(S) = f(V\backslash S)$ for all $S \subseteq V$
\item (Subadditive) $f(A \cup B) = f(A) + f(B)$ for all $A,B \subseteq V$.
\end{enumerate}
Consider the following integer program, to be labelled $IP$:
\begin{align*}
\text{min} \sum_{e \in E} d(e) x_e\\
\text{s.t.} \sum_{e\in \delta(S)} x_e &\geq f(S) &\forall S \subseteq V \\
x &\geq 0, \text{ integer}.
\end{align*}
\paragraph{Solving $IP$ of Trees}
We first observe that this $IP$ is easy to solve on trees. Indeed let $T$ be a tree, and for every $uv \in E(T)$ let $S^{uv}_u$ denote the set of vertices in connected component of $T-uv$ containing $u$, and define $S^{uv}_v$ similarly. By definition, $\delta(S^{uv}_u) =  \{e\} = \delta(S^{uv}_v)$. We define the following algorithm $\cA$ to solve $IP$ on a tree $T$:
\begin{enumerate}
\item For all $uv \in E(T)$ set $x_e = f(S^{uv}_u)$.
\item Return $x$.
\end{enumerate}
To see that $\cA$ solves $IP$ on trees, we will show that $(1)$ the solution returned is feasible, and $(2)$ that every feasible $x$ has necessarily $x_{uv} \geq f(S^{uv}_u)$, implying that the $x$ returned by $\cA$ is optimal.  Let $x$ be the solution returned by $\cA$. 
\paragraph{}
To see $(1)$, it is immediate from the definition of $f$ that $x \geq 0$. It remains to verify the cut inequalities. Let $S \subseteq V$. Enumerate $\delta(S)$ as $\delta(S) = \{e_1, \dots, e_k \}$, where each $e_i := u_iv_i$ and $u_i \in S$. Let $S_1, \dots, S_\ell$ be the partition of $S$ induced by the connected components of $G-\delta(S)$ contained in $S$.  Now let $D(S_i) = \{ e_j \in \delta(S) : u_j \in S_i \}$ be the set of edges in $\delta(S)$ incident with $S_i$. Notice that the set of $D(S_i)$ partition $\delta(S)$. 
\paragraph{}
We claim that $\bigcap_{e_j \in D(S_i)} S^{e_j}_{u_j} = S_i.$ If $v \in S_i$ then for all $e_j \in D(S_i)$ there exists a $vu_j$-path that does not use $e_j$, since $v$ and $u_j$ are in the same connected component $S_i$. Thus $v \in \bigcap_{e_j \in D(S_i)}S^{e_j}_{u_j}$. If $v \not\in S_i$ then either $v \in S_{k}$ for $k \neq i$ or $v \in V\backslash S$. In either case, $v$ is connected to any vertex in $S_i$ via a path using some $e_j \in D(S_i)$. But then $v \in S^{e_j}_{v_j} = V \backslash S^{e_j}_{u_j}$. Therefore $v \not\in \bigcap_{e_j \in D(S_i)}S^{e_j}_{u_j}$. Thus the claim holds.
\paragraph{}
So we have $$S = \bigcup_{i=1}^\ell S_i = \bigcup_{i=1}^\ell \bigcap_{e_j \in D(S_i)}S^{e_j}_{u_j}.$$ Now observe that
\begin{align*}
f(S) &\leq \sum_{i=1}^\ell f(\bigcap_{e_j \in D(S_i)}S^{e_j}_{u_j}) &\text{by subadditivity} \\
&= \sum_{i=1}^\ell f(V\backslash \bigcap_{e_j \in D(S_i)} S^{e_j}_{u_j}) &\text{by symmetry} \\
&= \sum_{i=1}^\ell f(\bigcup_{e_j\in D(S_i)} V\backslash S^{e_j}_{u_j}). \\
&\leq \sum_{i=1}^\ell \sum_{e_j \in D(S_i)} f(V\backslash S^{e_j}_{u_j}) &\text{by subadditivity} \\
&= \sum_{i=1}^\ell \sum_{e_j \in D(S_i)} f(S^{e_j}_{u_j}) &\text{by symmetry} \\
&= \sum_{e_j \in \delta(S)} f(S^{e_j}_{u_j}) \\
&= \sum_{e_j \i \delta(S)} x_{e_j}.
\end{align*}
Hence $x$ satisfies the cut constraint for $S$. Therefore $x$ is feasible and $(1)$ holds.
\paragraph{}
Now to demonstrate $(2)$, let $uv \in E(T)$. Consider the cut $S^{uv}_u$. We have $\delta(S^{uv}_u) = \{uv\}$ and hence by the cut constraint for $S^{uv}_u$, all feasible $x$ satisfy
$$x_{uv} \geq f(S^{uv}_u).$$
Now the $x$ we construct is tight for all the above, and since $d \geq 0$ we can do no better without violating feasibility. Thus $(2)$ holds and our $x$ is optimal. So we conclude that we can solve $IP$ on trees easily.
\paragraph{An Approximation Algorithm for $IP$}
We now give an algorithm that produces an integral solution to $IP$ for a general metric graph $G$ which approximates the optimal $LP$-relaxation of $IP$ within a factor of $O(\log n)$. Our algorithm will use the following theorem from class:
\begin{theorem} \label{th:metric-tree}
Let $G=(V,E)$ be a graph. Let $d: E \rightarrow \R_+$ be a metric function. Let $\lambda^*_{uv} \geq 0$ be coefficients for all $(u,v) \in V\times V$. There exists a polynomial time algorithm which constructs a weighted tree $T=(V,E)$ such that $d^T(u,v) \geq d(u,v)$ for all $u,v\in V$ and $$\sum_{u,v} \lambda^*_{uv}d^T(u,v) \leq O(\log n) \sum_{u.v} \lambda^*_{uv} d(u,v).$$
\end{theorem}
Consider the algorithm $\cB$:
\begin{enumerate}
\item Let $x^*$ be the optimal solution to the linear programming relaxation.
\item Use Theorem \ref{th:metric-tree} with $\lambda^* = x^*$ to obtain a weighted tree $T$.
\item Solve $IP$ on $T$ as discussed above and return the solution $\bar{x}$.
\end{enumerate}

\end{document}

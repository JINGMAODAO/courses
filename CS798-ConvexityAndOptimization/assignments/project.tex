\documentclass[letterpaper,12pt,oneside,onecolumn]{article}
\usepackage[margin=1in, bottom=1in, top=1in]{geometry} %1 inch margins
\usepackage{amsmath, amssymb, amstext}
\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{theorem}
\usepackage{tikz}
\usepackage{tkz-berge}

%Macros
\newcommand{\A}{\mathbb{A}} \newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}} \newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}} \newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
 
 
\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\newenvironment{proof}{{\bf Proof:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofof}[1]{{\bf Proof of #1:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofofnobox}[1]{{\bf#1:  }}{}\newenvironment{example}{{\bf Example:  }}{\hfill\rule{2mm}{2mm}}

%\renewcommand{\thesection}{\lecnum.\arabic{section}}
%\renewcommand{\theequation}{\thesection.\arabic{equation}}
%\renewcommand{\thefigure}{\thesection.\arabic{figure}}

\newtheorem{fact}{Fact}[section]
\newtheorem{lemma}[fact]{Lemma}
\newtheorem{theorem}[fact]{Theorem}
\newtheorem{definition}[fact]{Definition}
\newtheorem{corollary}[fact]{Corollary}
\newtheorem{proposition}[fact]{Proposition}
\newtheorem{claim}[fact]{Claim}
\newtheorem{exercise}[fact]{Exercise}
\newtheorem{note}[fact]{Note}
\newtheorem{conjecture}[fact]{Conjecture}

\newcommand{\size}[1]{\ensuremath{\left|#1\right|}}
\newcommand{\ceil}[1]{\ensuremath{\left\lceil#1\right\rceil}}
\newcommand{\floor}[1]{\ensuremath{\left\lfloor#1\right\rfloor}}

%END MACROS
%Page style
\pagestyle{fancy}

\listfiles

\raggedbottom

\lhead{2017-04-09}
\rhead{William Justin Toth CS798-Convexity and Optimization Project} %CHANGE n to ASSIGNMENT NUMBER ijk TO COURSE CODE
\renewcommand{\headrulewidth}{1pt} %heading underlined
%\renewcommand{\baselinestretch}{1.2} % 1.2 line spacing for legibility (optional)

\begin{document}
\paragraph{}
The multiplicative weights update method studied in Lecture $9$ is a powerful and ubiquitous tool in algorithm design \cite{arora2012multiplicative}. This method is the inspiration behind the critical step in the primal-dual paradigm for designing competitive online algorithms \cite{buchbinder2009design}. One major area of application for such algorithms is the design of auctions for selling ad space in search query results.
\paragraph{}
We will begin by introducing the framework in which competitive online algorithms are studied. General packing/covering problems with be defined, and a we will give an online algorithm using the primal-dual paradigm for such problems. We will observe how a multiplicative weights update procedure is used to update the primal variables in each iteration. 
\paragraph{}
Afterwards we will see how ideas from this general approach can be applied to applications in implementing ad auctions. We will start with the simple single-unit case, then move on to the more complex multi-slot case. In the multi-slot case strong duality of linear programs will be of great utility.
\section{Competitive Online Algorithms via Primal-Dual}
\subsection{Online Packing and Covering}
\paragraph{}
We will start by discussing packing/covering problems in the standard, offline setting then move from there to the online model. The $\textit{covering}$ problem is specified by a linear program $(P)$ of the form:
\begin{align*}
\min &\sum_{i=1}^n c_i x_i \\
\text{s.t.} \sum_{i \in S(j)} x_i &\geq 1 &\forall j \in [m]\\
x_i &\geq 0 &\forall  i \in [n].
\end{align*}
There is some notation here to unpack. First for any $k \in \N$ we have $[k] := \{1, \dots, k\}$. The vector $x \in \R^n$ is the variable set. The vector $c \in \R^n$ is the objective function, which is assumed to be non-negative. The sets $S(1), \dots, S(m)$ are subsets of $[n]$. We note here that this model is not the most general form of covering problem imaginable, but it is somewhat simpler while still being general enough to capture the main ideas we are striving for. 
\paragraph{}
We are discussing $\textit{fractional}$ covering here, but if we were to restrict our attention to $0-1$ integral solutions we would have nice a combinatorial interpretations for this problem. In this context, covering asks you to find a minimum cost subset of $[n]$ whose intersection with each $S(j)$ is non-empty.
\paragraph{}
The $\textit{packing}$ problem $(D)$ is defined as the dual to a corresponding covering problem:
\begin{align*}
\max &\sum_{j=1}^m y_j \\
\text{s.t.} \sum_{j : i \in S(j)} y_j &\leq c_i &\forall i \in [n] \\
y_j &\geq 0 &\forall j \in [m].
\end{align*}
\paragraph{}
In the $\textit{online covering problem}$ we are solving the covering problem, but we do not have all the information in advance. The cost function $c$ is known to us, as well as the size of the variable space. The entire space of constraints is not known. The constraints are given in some sequence unknown to the algorithm. At each point in the sequence the algorithm receives knowledge of one constraint, and must decide on a feasible solution to the covering problem with the constraints at hand. Any variables the algorithm decides to increase to maintain feasibility of the current sub-instance may not be decreased at future points in the input sequence.
\paragraph{}
One can observe that at any point in the operation of the algorithm in the online model, the constraints revealed so far form a sub-instance of the covering problem which is itself a covering problem. We define the online packing problem in such a way that its sub-instances are the duals to the covering problem sub-instances. That is, we reveal the variables one-at-a-time in the online packing problem, as opposed to the constraints as we did for online covering.
\paragraph{}
In the $\textit{online packing problem}$ we are solving the packing problem, but again we do not have all the information in advance. The values of $c$ are all known to us in advance, however the number of variables and the precise way they must be packed is not known. At each point in the sequence for which the input is given, a new variable can be introduced to the algorithm. If $y_j$ is the variable introduced, the algorithm is at this point made aware of precisely which $i \in [n]$ satisfy $i \in S(j)$. So the constraints are revealed over time. The algorithm must decide on the value of $y_j$ at the point it is introduced and may not change it in subsequent iterations. Under this definition, for any online covering problem, there is a corresponding online packing problem that, if run simultaneously, have their sub-instances at each point in the sequence the input is given acting as primal-dual pairs.
\subsection{General Algorithm}
\paragraph{}
Consider the following algorithm for simultaneously solving a corresponding pair of online packing/covering problems of the form $(P)$ and $(D)$ from the previous subsection. By scaling, we may assume that each $c_i \geq 1$. We denote the algorithm by $\cA$:
\begin{enumerate}
\item When a new constraint for $(P)$ of the form $\sum_{i \in S(j)} x_i \geq 1$ arrives with corresponding dual variable $y_j$ do:
\item While $\sum_{i \in S(j)} x_i < 1$:
	\begin{enumerate}
	\item For each $i \in S(j)$: $x_i \leftarrow x_i(1+\frac{1}{c_i}) + 1(|S(j)|\cdot c_i)$.
	\item $y_j \leftarrow y_j +1$.
	\end{enumerate}
\end{enumerate}
\paragraph{}
We now proceed to analyze this algorithm. The critical notion we will use here is that of a $\textit{competitive online algorithm}$. An online algorithm $\cA$ (or its solution) is said to be $c$-competitive if for every instance of the minimization problem $\cA$ solves, the cost of the solution produced by $\cA$ is at most $c\cdot OPT + \alpha$ where $OPT$ is the optimal value of solving the ``offline" version of the problem, and $\alpha$ is an additive factor independent of the sequence in which the input is presented to $\cA$. If $\cA$ instead solves a maximization problem, to be $c$-competitive the cost of the solution produced instead needs to be at least $\frac{1}{c} OPT -\alpha$.
\begin{theorem}\label{th:1}
Let $d = \max_{j \in [m]} |S_(j)|$. Then $\cA$ produces two things:
\begin{enumerate}
\item An $O(\log d)$-competitive fractional covering solution,
\item and a $2$-competitive integral packing solution which violates each packing constraint by at most $O(\log d)$.
\end{enumerate}
Note: We can rescale our packing solution by $O(\log d)$ to obtain a feasible fractional packing if desired.
\end{theorem}
\begin{proof}
The dual variables start at $0$ and increase by $1$ whenever they are changed. Thus the packing solution returned is integral. By an iteration of $\cA$ we mean a complete execution of step $2$. Let $x$ be the solution for $(P)$ returned by $\cA$. Let $y$ be the solution for $(D)$ returned by $\cA$. We want to show three things:
\begin{enumerate}
\item $x$ is feasible for $(P)$.
\item In each iteration of $\cA$ the increase in $c^Tx$ is at most twice the increase in $\sum_{j=1}^m y_j$.
\item for all $i \in [n]$, $\sum_{j : i \in S(j)} y_j \leq c_i \cdot O(\log d)$.
\end{enumerate}
\paragraph{}
Part $1.$ is quite clear. Whenever a primal constraint is introduced, the While loop of Step $2$ does not terminate until the constraint is satisfied. Subsequent iterations do not decrease the variables of $x$, so the constraint remains satisfied.
\paragraph{}
To demonstrate $2.$ we will study a single iteration of the While loop. In an iteration of Step $2$ the change in $\sum_{j=1}^m y_j$ is precisely $1$. The change in the primal objective value $c^Tx$ is (from the update in Step $2a$):
$$\sum_{i \in S(j)} c_i (\frac{x_i}{c_i} + \frac{1}{|S(j)| c_i}) = \sum_{i \in S(j)}(x_i + \frac{1}{|S(j)|}) \leq 2$$
since $\sum_{i \in S(j)} x_i \leq 1$ at the time Step $2a$. Therefore $2.$ holds.
\paragraph{}
Let $i \in [n]$. We consider the dual constraint corresponding to $i$. Whenever we increase a dual variable $y_j$ such that $i \in S(j)$ by $1$ in Step $2b$ we had just increased $x_i$ in Step $2a$. We claim that 
$$x_i \geq \frac{1}{d}((1+\frac{1}{c_i})^{\sum_{j : i \in S(j)} y_j} - 1).$$
We will proceed by induction on the iterations of $\cA$. In the base case $x_i = 0$, and $\sum_{j : i \in S(j)} y_j = 0$ so the claim holds. Now consider an iteration in which some dual variable $y_k$ with $k \in S(j)$ is increased. Let $x_i'$ denote the value of $x_i$ after this iteration (and $x_i$ denote the value before).
\begin{align*}
x'_i &= x_i(1 + \frac{1}{c_i}) + \frac{1}{|S(j)|c_i} \\
&\geq x_i(1+\frac{1}{c_i}) + \frac{1}{d\cdot c_i} \\
&\geq \frac{1}{d}((1+\frac{1}{c_i})^{\sum_{j : i \in S(j)\backslash\{k\}}y_j} - 1)(1 + \frac{1}{c_i})^y_k + \frac{1}{d\cdot c_i} &\text{by the inductive hypothesis on $x_i$} \\
&= \frac{1}{d}((1+\frac{1}{c_i})^{\sum_{j :i \in S(j)} y_j} -1).
\end{align*}
So the claim holds by induction. Now we compute an upper bound on $x'_i$. By the condition of the While loop, $x_i <1$. We also have $c_i \geq 1$ and $d\geq 1$, so
$$x'_i =  x_i(1 + \frac{1}{c_i}) + \frac{1}{|S(j)|c_i} \leq 1(1+1) + 1 = 3.$$
Combining inequalities we have
$$ \frac{1}{d}((1+\frac{1}{c_i})^{\sum_{j :i \in S(j)} y_j} -1) \leq 3.$$
Using once more that $c_i \geq 1$ and rearranging for $\sum_{j : in \in S(j)} y_j$ we obtain
$$\sum_{j : i \in S(j)} y_j \leq c_i \log(3d+1) = c_i \cdot O(\log d)$$
as desired.
\paragraph{}
It remains to use these claims to verify the competitive factors. This is done via duality of linear programs. Let $p^*$ be the optimal value of $(P)$. Let $f^*$ be the optimal value of $(D)$. By claim $2$ we have
$$c^Tx \leq 2 \sum_{j} y_j.$$
Since $x$ is primal feasible, $c^Tx \geq p^*$. So 
$$2\sum_j y_j \geq c^Tx \geq p^* = f^*.$$
Thus $y$ is $2$-competitive. By claim $3$, $y' \in \R^m$ where $y'_j = y_j/O(\log d)$ is feasible for $(D)$.
So similarly,
$$c^Tx \leq 2 O(\log d) \sum_j y_j \leq O(\log d) f^* = O(\log d) p^*.$$
Therefore $x$ is $O(\log d)$-competitive.
\end{proof}
\paragraph{Connection to Multiplicative Weights}
If we looks at Step $2a$ of $\cA$, we observe a multiplicative update rule very similar to those used in the study of the Multiplicative Weights Update Method of Lecture $9$:
$$x_i \leftarrow x_i(1+\frac{1}{c_i}) + 1(|S(j)|\cdot c_i).$$
So each $x_i$ involved in constraint $j$ is increased multiplicatively, while $y_j$ increases additively during the operation of the While loop in Step $2$. Notice that during the proof for the competitive factor we used an induction obtaining that $x_i$ is approximately an exponential function of the $y_j$'s:
$$x_i = \frac{1}{d}((1+\frac{1}{c_i})^{\sum_{j : i \in S(j)} y_j} -1),$$
similar to how we observe an exponential relationship between the potential function (in the proof for the performance guarantee in multiplicative weights update) and the variables of the multiplicative weights update procedure. In both scenarios these exponentials that arise from the respective multiplicative update rules allow us to derive logarithmic performance factors. The difference being that factor is additive in multiplicative update method, and is multiplicative in primal-dual.
\section{Ad Auctions}
\paragraph{}
The business of displaying ads in search engine results is a natural and important application for online algorithms. Search provides like Google and Microsoft will sell ad space to advertisers in the search results corresponding with some users given keyword query. The way they sell the space is through auctions to buyers interested in displaying ads related to the given query.

\subsection{Single Slot}
\paragraph{}
In the $\textit{ad-auctions problem}$ we are given a set $I$ of $n$ buyers, each with a daily budget of $B(i)$. We are also given a set $M$ of items with $|M| = m$. The items arrive in an online manner. Upon the arrival of some item, say the $j$-th item for $j \in [m]$, each buyer will submit a bid for said item, denoted $b(i,j)$. The algorithm can decide to allocate the item to any of the buyer upon its arrival. The online constraint is that they cannot reallocate items once they have decided who they are to be allocated to.
\paragraph{}
Allocations can be either $\textit{fractional}$ or $\textit{integral}$. In integral allocation, each item is allocated to a single buyer. In fractional allocations the items can be subdivided and allocated to multiple buyers. The algorithm we present will give an integral allocation, but the linear programming formulation we base it on will be fractional. Clearly, integral allocations suffice as fractional allocations, so we can naturally come our integral solutions to the optimal values of the fractional linear programs.
\paragraph{}
The objective is to maximize the total revenue of the seller. The revenue received from each buyer is the minimum of the sum total of bids corresponding to items allocated to that buyer and their budget. It is defined as such since we cannot overcharge buyers.
\subsection{Multiple Slots}
\bibliography{references}
\bibliographystyle{plain}
\end{document}

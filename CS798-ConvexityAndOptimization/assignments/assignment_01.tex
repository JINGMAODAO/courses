\documentclass[letterpaper,12pt,oneside,onecolumn]{article}
\usepackage[margin=1in, bottom=1in, top=1in]{geometry} %1 inch margins
\usepackage{amsmath, amssymb, amstext}
\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{theorem}
\usepackage{tikz}
\usepackage{tkz-berge}

%Macros
\newcommand{\A}{\mathbb{A}} \newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}} \newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}} \newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
 
 
\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\newenvironment{proof}{{\bf Proof:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofof}[1]{{\bf Proof of #1:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofofnobox}[1]{{\bf#1:  }}{}\newenvironment{example}{{\bf Example:  }}{\hfill\rule{2mm}{2mm}}

%\renewcommand{\thesection}{\lecnum.\arabic{section}}
%\renewcommand{\theequation}{\thesection.\arabic{equation}}
%\renewcommand{\thefigure}{\thesection.\arabic{figure}}

\newtheorem{fact}{Fact}[section]
\newtheorem{lemma}[fact]{Lemma}
\newtheorem{theorem}[fact]{Theorem}
\newtheorem{definition}[fact]{Definition}
\newtheorem{corollary}[fact]{Corollary}
\newtheorem{proposition}[fact]{Proposition}
\newtheorem{claim}[fact]{Claim}
\newtheorem{exercise}[fact]{Exercise}
\newtheorem{note}[fact]{Note}
\newtheorem{conjecture}[fact]{Conjecture}

\newcommand{\size}[1]{\ensuremath{\left|#1\right|}}
\newcommand{\ceil}[1]{\ensuremath{\left\lceil#1\right\rceil}}
\newcommand{\floor}[1]{\ensuremath{\left\lfloor#1\right\rfloor}}

%END MACROS
%Page style
\pagestyle{fancy}

\listfiles

\raggedbottom

\lhead{2017-02-15}
\rhead{William Justin Toth CS798-Convexity and Optimization Assignment 1} %CHANGE n to ASSIGNMENT NUMBER ijk TO COURSE CODE
\renewcommand{\headrulewidth}{1pt} %heading underlined
%\renewcommand{\baselinestretch}{1.2} % 1.2 line spacing for legibility (optional)

\begin{document}
%Q1%
\section{}
\paragraph{}
Let $q > p > 0$ and let $x_1, \dots, x_n \in \R_{>0}$. Let $f : \R_{>0} \rightarrow \R_{>0}$ be the function given by $f(x) = x^\frac{q}{p}$. We claim that $f$ is convex. To see this we will use the second order condition ($f'' \geq 0$). We have by calculus that
$$f' (x)= \frac{q}{p} x^{\frac{q}{p}-1}$$
and thus 
$$f''(x) = \frac{q}{p}(\frac{q}{p} -1) x^{\frac{q}{p} - 2}.$$
Now since $q > p$, we have $\frac{q}{p} > 1$. So then $\frac{q}{p} - 1$ is non-negative, and further $x^{\frac{q}{p} - 2} > 0$ since $x >0$. Hence $f''(x) \geq 0$ on the domain of $f$. Therefore by the second order condition, $f$ is convex.
\paragraph{}
Now observe by convexity that
$$f(\frac{1}{n}\sum_{i=1}^n x_i^p) \leq \frac{1}{n} \sum_{i=1}^n f(x_i^p) = \frac{1}{n} \sum_{i=1}^n x_i^q.$$
On the other hand, by applying $f$ directly we see that
$$f(\frac{1}{n}\sum_{i=1}^n x_i^p) = \frac{1}{n}^\frac{q}{p}(\sum_{i=1}^n x_i^p)^\frac{q}{p}.$$
Combining the previous two expressions yields the inequality
$$\frac{1}{n}^\frac{q}{p}(\sum_{i=1}^n x_i^p)^\frac{q}{p} \leq \frac{1}{n} \sum_{i=1}^n x_i^q.$$
By raising both sides to the exponent $\frac{1}{q}$ (which preserves order) we obtain that
$$(\frac{1}{n}\sum_{i=1}^n x_i^p)^\frac{1}{p} \leq (\frac{1}{n} \sum_{i=1}^n x_i^q)^\frac{1}{q}$$
as desired. $\blacksquare$
%Pa
\subsection{a}
\paragraph{}
Consider the function
$$f(x) = (\sum_{i=1}^n x_i^p)^\frac{1}{p}$$
with $\text{dom} f = \R^n_{>0}$. We will compute the Hessian matrix. First observe that
$$\frac{\partial f}{\partial x_i} f = x_i^{p-1}(\sum_{i=1}^n x_i^p)^\frac{1-p}{p}$$
for $i = 1, \dots, n$. Now for $j = 1, \dots, n$
\begin{align*}
\frac{\partial f}{\partial x_j}\frac{\partial f}{\partial x_i} f &= \frac{\partial f}{\partial x_j}   x_i^{p-1}(\sum_{i=1}^n x_i^p)^\frac{1-p}{p} \\
&= \begin{cases}
0, &\text{ if } i=j \\
(p-1)x_i^{p-1}(\sum_{i=1}^n x_i^p)^\frac{1-p}{p}[1- \frac{x_i^{p-1}}{\sum_{i=1}^n x_i^p}], &\text{if } i \neq j.
\end{cases}
\end{align*}
Observe that if $p \geq 1$ each factor in the second case is non-negative when $x \in \R^n_{>0}$. Hence $\frac{\partial f}{\partial x_i} f  \geq 0$ for all $x \in \text{dom f}$ for all $i,j = 1, \dots, n$. Thus each entry of the Hessian is non-negative, and so the Hessian is PSD. Thus by the second order condition $f$ is convex.
\paragraph{}
Now when $p < 1$, by a similar calculation we observe
$$\frac{\partial f}{\partial x_i} -f = \begin{cases}
0, &\text{ if } i=j \\
(1-p)x_i^{p-1}(\sum_{i=1}^n x_i^p)^\frac{1-p}{p}[1- \frac{x_i^{p-1}}{\sum_{i=1}^n x_i^p}], &\text{if } i \neq j.
\end{cases}$$
Again each factor is non-negative, and hence $-f$ is convex by the second order condition. Therefore $f$ is concave. $\blacksquare$
\paragraph{}
Next we show that
$$||x||_p = (\sum_{i=1}^n |x_i|^p)^\frac{1}{p}$$
is a norm for $p\geq 1$. It is easy to see that $||x||_p$ is non-negative since it is a sum of absolute values. Similarly if any $x_i > 0$ then $||x||_p > 0$, so $||x||_p = 0$ if and only if $x = 0$. Now consider some scalar $\alpha$. We have
$$||\alpha x||_p = (\sum_{i=1}^n |\alpha x_i|^p)^\frac{1}{p} = (\sum_{i=1}^n |\alpha|^p|\cdot |x_i|^p)^\frac{1}{p} = |\alpha|(\sum_{i=1}^n |x_i|^p)^\frac{1}{p} = |\alpha|\cdot ||x||_p.$$
\paragraph{}
Lastly it remains to verify the triangle inequality. Let $x, y \in \R^n$. If either $x$ or $y$ is $0$, say without loss of generality $y=0$, then 
$$||x + y||_p = ||x||_p + 0 = ||x||_p + ||y||_p$$
immediately. So we may assume $x$ and $y$ are non-zero. Thus it suffices to prove
$$f(x+y) \leq f(x) + f(y).$$
To show this we exploit convexity of $f$ as follows
$$f(x+y) = f(\frac{1}{2}(2x+2y)) \leq \frac{1}{2}(f(2x) + f(2y)) = \frac{1}{2} ((2^p\sum_{i=1}^n x^p)^\frac{1}{p} +(2^p\sum_{i=1}^n y^p)^\frac{1}{p}) = f(x) + f(y).$$
Therefore $||x + y||_p \leq ||x||_p + ||y||_p$, and hence $||x||_p$ is a norm. $\blacksquare$
\paragraph{}
Finally we give a counter-example showing $||x||_p$ is not a norm when $0 < p < 1$. Consider $p =\frac{1}{2}$ and $e_1, e_2 \in \R^2$ the standard basis vectors of $\R^2$. We prove $||x||_p$ violates the triangle inequality. Observe
$$||e_1 + e_2 ||_p = (1 + 1)^2 = 4$$
while 
$$||e_1|| = (1+0)^2 = 1 = ||e_2||$$
hence 
$$||e_1 + e_2||  = 4 < 2 = ||e_1|| + ||e_2||.$$
Thus the triangle inequality is violated, and so $||x||_p$ is not a norm.
%Pb
\subsection{b}
%Pc
\subsection{c}
%Q2
\section{}

%Q3
\section{}
\paragraph{}
Let $G = (V,E)$ be a graph. For any $S, T \subseteq V$ we denote by $E(S,T)$ the set of edges between $S$ and $T$. Formally $E(S,T) = \{\{s,t\} \in E: s \in S, t \in T\}$.
%Pa
\subsection{a}

%Pb
\subsection{b}
\paragraph{}
Our goal is to find a maximum $k$-Cut of $G$. That is, we seek to find a partition of $V$ into $V_1, \dots, V_k$ maximizing
$$\sum_{i=1}^k \sum_{j = 1}^{i-1} |E(V_i, V_j)| := c(\{V_1, \dots, V_k\}).$$
Intuitively we are maximizing the edges between the selected cuts. Let $\cB$ denote the $2$-approximation algorithm for max cut given in problem $3a$. Consider the following algorithm, which we will denote by $\cA$:
\begin{enumerate}
\item Set $V_1 = S$ and $V_2 = V\backslash S$ where $S$ is the cut returned by $\cB$ when run on $G$.
\item Set $V_i = \emptyset$ for $i = 3, \dots, k$.
\item For $i = 2, \dots, k-1$
\begin{enumerate}
\item For $j = 1, \dots, i$ set $S_j$ to the cut returned by $\cB$ when run on $G[V_j]$.
\item Set $j^* = \text{argmax}_{j=1,\dots, i} \{|E(S_j, V_j\backslash S_j)|\}$.
\item Set $V_{i+1} = S_{j^*}$ and set $V_{j^*} = V_{j^*} \backslash S_{j^*}$.
\end{enumerate}
\item Return $V_1, \dots, V_k$.
\end{enumerate}
The idea behind the operation of $\cA$ is to start with a $2$-cut of $G$, and in every successive iteration take the current partition and $2$-cut one of the parts in an approximately optimal way until we reach a $k$-cut.
\begin{lemma}\label{lemma:3b1}
Let $\cO = \{V^*_1, \dots, V^*_k\}$. Consider an arbitrary iteration $i = 1, \dots, k-1$ (iteration $1$ is original $2$-cut). Let $\cS = \{V_1, \dots, V_{i+1}\}$ be the set of partitions chosen so far by $\cA$. Suppose that at the end of iteration $i$, the algorithm has split $V_j$ into $V_j'$ and $V_{i+1}$. That is the solution after iteration $i$ is
$$\cS' = \cS \backslash V_j \cup \{V_j', V_{i+1}\}.$$
Then 
$$2k(c(\cS') - c(\cS)) \geq c(\cO) - c(\cS).$$
\end{lemma}
\begin{proof}
The difference between $c(\cS')$ and $c(\cS)$ is precisely $|E(V_j', V_{i+1})|$ (the number of edges gained by splitting $V_j$). Further the difference $c(\cO) - c(\cS)$ is upper-bounded by the edges counted in $c(\cO)$ that are not counted in $c(\cS)$. Call such edges $E'$.  The edges $E'$ are necessarily internal to a part of $\cS$ (that is, both endpoints lie in some $V_k \in \cS$). Otherwise they are counted in $c(\cS)$ since their endpoints lie in distinct parts of $\cS$. Thus if we consider the $k$-cut induced by $\cO$ on $G[E']$ it consists of at most $k$ splits ($2$-cuts) of parts $V_k \in \cS$.
\paragraph{}
Let $V'_k$ denote the best possible $2$-cut of $V_k \in \cS$ for all $k = 1, \dots, i$. Now let $$k^* = \text{argmax}_{k=1,\dots,i}  \{|E(V'_k, V_k \backslash V'_k)|\}.$$ We have
$$ |E'| \leq k |E(V'_{k^*}, V_k \backslash V'_{k^*})|$$
by our previous discussion, since $E'$ edges are formed by a $k$-cut on edges internal to parts of $\cS$, and  $|E(V'_{k^*}, V_k \backslash V'_{k^*}|$ is largest $2$-cut to be formed by edges internal to a part of $\cS$. Now the cut found by our algorithm $\cA$ in this iteration in step $3(c)$ is a $2$-approximation of the best possible split cut. That is,
$$|E(V'_{k^*}, V_k \backslash V'_{k^*})| \leq 2|E(V_j', V_{i+1})| .$$
Hence, combining these inequalities, we see that
$$|E'| \leq 2k |E(V_j', V_{i+1})|.$$
Therefore $2k(c(\cS') - c(\cS)) \geq c(\cO) - c(\cS)$ as desired.
\end{proof}
\paragraph{}
Using the previous lemma, we can demonstrate that $\cA$ is a $\alpha := 1-(1-\frac{1}{2k})^k$-approximation algorithm for the max $k$-cut problem.
\paragraph{Main Proof}
Since $\cB$ can be run in polynomial time, and $k$ is at most $|V|$, it is clear that $\cA$ runs in polynomial time. At termination the algorithm returns a partition of $V$ into $k$ parts so it returns a feasible solution.
\paragraph{} 
Now to see the approximation factor holds let $\cS^i$ denote the solution maintained by $\cA$ at the end of iteration $i$. Then the returned solution is $\cS^{k-1}$. Let $\cO$ be an optimal solution. Observe from Lemma \ref{lemma:3b1} that $c(\cS^{i}) \geq \frac{1}{2k} c(\cO) + (1- \frac{1}{2k})c(S^{i-1})$. We will apply this inequality inductively to achieve the bound:
\begin{align*}
c(\cS^{k-1} &\geq \frac{1}{2k} c(\cO) + (1- \frac{1}{2k})c(S^{k-2}) \\
		&\geq \frac{1}{2k} c(\cO) + (1- \frac{1}{2k})(\frac{1}{2k} c(\cO) + (1- \frac{1}{2k})c(S^{k-3})) \\
		&\geq \frac{1}{2k} c(\cO) + (1- \frac{1}{2k})(1 + (1-\frac{1}{2k}) + (1-\frac{1}{2k})^2 + \dots + (1-\frac{1}{2k})^{k-1}) \\
		&= \frac{c(\cO)}{2k} \cdot \frac{1-(1-\frac{1}{2k})^k}{1-(1-\frac{1}{2k})} \\
		&=\frac{c(\cO)}{2k} \cdot \frac{1-(1-\frac{1}{2k})^k}{\frac{1}{2k}}\\
		&= 1-(1-\frac{1}{2k})^kc(\cO).
\end{align*}
Thus the approximation factor holds as desired. $\blacksquare$
\end{document}

\documentclass[letterpaper,12pt,oneside,onecolumn]{article}
\usepackage[margin=1in, bottom=1in, top=1in]{geometry} %1 inch margins
\usepackage{amsmath, amssymb, amstext}
\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{theorem}
\usepackage{tikz}
\usepackage{tkz-berge}

%Macros
\newcommand{\A}{\mathbb{A}} \newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}} \newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}} \newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
 
 
\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\newenvironment{proof}{{\bf Proof:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofof}[1]{{\bf Proof of #1:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofofnobox}[1]{{\bf#1:  }}{}\newenvironment{example}{{\bf Example:  }}{\hfill\rule{2mm}{2mm}}

%\renewcommand{\thesection}{\lecnum.\arabic{section}}
%\renewcommand{\theequation}{\thesection.\arabic{equation}}
%\renewcommand{\thefigure}{\thesection.\arabic{figure}}

\newtheorem{fact}{Fact}[section]
\newtheorem{lemma}[fact]{Lemma}
\newtheorem{theorem}[fact]{Theorem}
\newtheorem{definition}[fact]{Definition}
\newtheorem{corollary}[fact]{Corollary}
\newtheorem{proposition}[fact]{Proposition}
\newtheorem{claim}[fact]{Claim}
\newtheorem{exercise}[fact]{Exercise}
\newtheorem{note}[fact]{Note}
\newtheorem{conjecture}[fact]{Conjecture}

\newcommand{\size}[1]{\ensuremath{\left|#1\right|}}
\newcommand{\ceil}[1]{\ensuremath{\left\lceil#1\right\rceil}}
\newcommand{\floor}[1]{\ensuremath{\left\lfloor#1\right\rfloor}}

%END MACROS
%Page style
\pagestyle{fancy}

\listfiles

\raggedbottom

\lhead{\today}
\rhead{W. Justin Toth CS761-Randomized Algorithms Assignment 1} %CHANGE n to ASSIGNMENT NUMBER ijk TO COURSE CODE
\renewcommand{\headrulewidth}{1pt} %heading underlined
%\renewcommand{\baselinestretch}{1.2} % 1.2 line spacing for legibility (optional)

\begin{document}
\section{}
\paragraph{}
We modify Karger's algorithm as follows:
\begin{enumerate}
\item While there are more than $\bf{k}$ vertices in the graph
	\begin{itemize}
		\item Pick an edge uniformly at random and contract the two endpoints.
	\end{itemize}
\item Output the edges between the remaining $k$ vertices.
\end{enumerate}
\begin{lemma}
Let $G=(V,E)$ be a graph. The probability that the algorithm outputs a minimum $k$-way cut in $G$ is at least $$\frac{2(k-1)}{n(n-1)\dots(n-2(k-1)+1)}$$ where $n = |V|$.
\end{lemma}
\begin{proof}
Let $F\subseteq E$ be a minimum $k$-way cut in $G$. Let $G^i=(V^i,E^i)$ be the contracted graph at iteration $i$ of Karger's algorithm. By our stopping condition $|V^i| \geq k$. Since a cut in the contracted graph at each iteration is a cut in the original graph, the minimum $k$-way cut value of $G^i$ is at least $|F|$.
\paragraph{}
Let $S\subseteq V^i$ such that $|S|=k-1$. Then $\sum_{v \in S} |\delta(v)| \geq |F|$, for otherwise we have a cut $\bigcup_{v\in S} \delta(v)$ of size smaller than $F$ in $G^i$. Thus by partitioning the vertices of $G^i$ into sets of size $k-1$ we see that
$$|E^i| \geq \floor{\frac{(n-i+1)|F|}{2(k-1)}}.$$
Hence the probably that we pick an edge to contract in iteration $i$ from $F$ is at most
$$ \frac{|F|}{\frac{(n-i+1)|F|}{2(k-1)}} \leq \frac{2(k-1)}{n-i+1}.$$
Therefore the probability that $F$ survives until the end of the algorithm is at least
$$(1-\frac{2(k-1)}{n})(1-\frac{2(k-1)}{n-1})(1-\frac{2(k-1)}{n-2})\dots(1-\frac{2(k-1)}{k+1}) = \frac{2(k-1)}{n(n-1)\dots(n-2(k-1)+1)}$$
as desired.
\end{proof}
\paragraph{}
Using this lemma, we observe that if we repeat Karger's algorithm $t$ times, and take the best cut we find, then our failure probability is at most 
$$\big(1 -  \frac{2(k-1)}{n(n-1)\dots(n-2(k-1)+1)}\big)^t \leq e^\frac{-2(k-1)t}{n(n-1)\dots(n-2(k-1)+1)}.$$
So we can achieve a fixed failure probability of at most $\epsilon \in (0,1)$ if we take 
$$t = \ceil{\frac{\ln(\epsilon)}{-2(k-1)}}n(n-1)\dots(n-2(k-1)+1) \leq n^{ck}\cdot n^{2(k-1)+1} \leq n^{O(k)}$$
for some fixed constant $c$ depending on $\epsilon$.

\newpage
\section{}
\paragraph{}
Let $i > m$. Let $H(i)$ be the event that you hire candidate $i$. Let $B(i)$ be the event that candidate $i$ is the best candidate. Then $E_i = H(i) \cap B(i)$. So using Bayes formula we see that
$$Pr(E_i) = Pr(H(i) \mid B(i)) \cdot Pr(B(i)).$$
Now since the candidates are distributed uniformly at random $Pr(B(i)) = \frac{1}{n}$.
\begin{lemma}
Let $F(i)$ be the event that the best candidate among the first $i$ candidates appears in the first $m$ candidates. Then
$$Pr(H(i) \mid B(i)) = Pr(F(i-1)).$$
\end{lemma}
\begin{proof}
\paragraph{}
Given that $i$ is the best candidate, we hire $i$ if and only if we reject the first $i-1$ candidates. If we reject the first $i-1$ candidates then the best candidate among the first $i-1$ appears in the first $m-1$ candidates. Otherwise, it would not be rejected when it appears, as it is better than all previously seen candidates. On the other hand if the best candidates among the first $i-1$ appears in the first $m$ candidates, then we will reject the first $i-1$ candidates as none of them are better than a candidate which appears in the first $m$. Thus $H(i)$ happens given $B(i)$ if and only if $F(i-1)$ happens, i.e. the claim holds.
\end{proof}
\paragraph{}
Using this lemma we have
$$Pr(H(i) \mid B(i)) = Pr(F(i-1)) = \frac{m}{i-1}$$
since there are $m$ chances among $i-1$ that the best candidate among the first $i-1$ appears in the first $m$.
Therefore $Pr(E_i) = \frac{m}{n(i-1)}$.
\paragraph{}
Now the event $E$ is the disjoint union of events $E_1, \dots, E_n$. So we observe 
$$Pr(E) = Pr(\bigcup_{i} E_i) = \sum_{i} Pr(E_i) = \frac{m}{n}\sum_{i=m+1}^n \frac{1}{i-1}$$
with the last equality following from our previous observations and the fact that $Pr(E_i) = 0$ for $i \leq m$.
\paragraph{}
We want to show that $Pr(E)$ can get arbitrarily close to $\frac{1}{e}$ for an appropriate choice of $m$. First we see that
$$Pr(E) =  \frac{m}{n}\sum_{i=m+1}^n \frac{1}{i-1} = \frac{m}{n}(\sum_{j=1}^{n-1}\frac{1}{j} - \sum_{j=1}^{m-1}\frac{1}{j}) =\frac{m}{n}(H_{n-1} - H_{m-1}).$$
Using that $\ln(n) \leq H_n \leq \ln(n+1)$ we have
$$Pr(E) \geq \frac{m}{n}(\ln(n-1) - \ln(m)) = \frac{m}{n}\ln(\frac{n-1}{m}).$$
If we choose $m = \frac{n-1}{e}$ then
$$Pr(E) \geq \frac{1}{e}\cdot\frac{n-1}{n}\ln(e) = \frac{1}{e}\cdot\frac{n-1}{n}$$
and taking the limit as $n$ tends to $\infty$ yields
$$\lim_{n\rightarrow \infty} Pr(E) \geq \frac{1}{e}.$$
\newpage
\section{}
\paragraph{}
Let $X$ be a random variable counting the number of cycles in a random permutation. Let $X_i$ be a random variable counting the number of cycles of length $i$ in a permutation. Then $X = \sum_i X_i$. So by linearity of expectation
$$E[X] = \sum_i E[X_i].$$
Now let $X_i^j$ be an indicator variable for $j \in {n\choose i}$ forms a length $i$ cycle in the random permutation. Then 
$$Pr(X^j_i) = \frac{(i-1)!(n-i)!}{n!}$$
since there are $n!$ permutations, and $(i-1)!(n-i)!$ permutations where $j$ forms a length $i$ cycle. To see this, consider a permutation where $j$ forms a length $i$ cycle. There are $(i-1)!$ ways to arrange the elements of $j$ in a length $i$ cycle, and $(n-i)!$ ways to permute the other elements. Therefore
$$E[X_i] = \sum_{j \in {n\choose i}}E[X_i^j] = {n\choose i} \frac{(i-1)!(n-i)!}{n!} = \frac{1}{i}.$$
Then
$$E[X] = \sum_{i=1}^n E[X_i] = \sum_{i=1}^n \frac{1}{i} = H_n.$$
\newpage
\section{}
\end{document}

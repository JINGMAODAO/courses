\documentclass[letterpaper,12pt,oneside,onecolumn]{article}
\usepackage[margin=1in, bottom=1in, top=1in]{geometry} %1 inch margins
\usepackage{amsmath, amssymb, amstext}
\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{theorem}
\usepackage{tikz}
\usepackage{tkz-berge}

%Macros
\newcommand{\A}{\mathbb{A}} \newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}} \newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}} \newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
 
 
\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\newenvironment{proof}{{\bf Proof:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofof}[1]{{\bf Proof of #1:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofofnobox}[1]{{\bf#1:  }}{}\newenvironment{example}{{\bf Example:  }}{\hfill\rule{2mm}{2mm}}

%\renewcommand{\thesection}{\lecnum.\arabic{section}}
%\renewcommand{\theequation}{\thesection.\arabic{equation}}
%\renewcommand{\thefigure}{\thesection.\arabic{figure}}

\newtheorem{fact}{Fact}[section]
\newtheorem{lemma}[fact]{Lemma}
\newtheorem{theorem}[fact]{Theorem}
\newtheorem{definition}[fact]{Definition}
\newtheorem{corollary}[fact]{Corollary}
\newtheorem{proposition}[fact]{Proposition}
\newtheorem{claim}[fact]{Claim}
\newtheorem{exercise}[fact]{Exercise}
\newtheorem{note}[fact]{Note}
\newtheorem{conjecture}[fact]{Conjecture}

\newcommand{\size}[1]{\ensuremath{\left|#1\right|}}
\newcommand{\ceil}[1]{\ensuremath{\left\lceil#1\right\rceil}}
\newcommand{\floor}[1]{\ensuremath{\left\lfloor#1\right\rfloor}}

%END MACROS
%Page style
\pagestyle{fancy}

\listfiles

\raggedbottom

\lhead{\today}
\rhead{W. Justin Toth CS761-Randomized Algorithms Assignment 2} %CHANGE n to ASSIGNMENT NUMBER ijk TO COURSE CODE
\renewcommand{\headrulewidth}{1pt} %heading underlined
%\renewcommand{\baselinestretch}{1.2} % 1.2 line spacing for legibility (optional)

\begin{document}
%Problem 1
\section{}
\paragraph{}
Let $v_1, \dots, v_m \in \{0,1\}^\ell$ be a set of vectors such that any subset of size at most $k$ is linearly independent modulo $2$. We denote the $j$-th entry of $v_i$ by $v_{i,j}$. Let $u$ be chosen uniformly at random from $\{0,1\}^\ell$. Let $X_i$ be the random variable defined by
$$X_i = (\sum_{j=1}^\ell v_{i,j}u_j) \mod 2.$$
We want to show that $X_1, \dots, X_m$ are $k$-wise independent random bits. That is to say, for all $I \subseteq [m]$ with $1\leq |I| \leq k$, and $x \in \{0,1\}^m$,
$$Pr(\bigcap_{i\in I} X_i = x_i) = \Pi_{i\in I} Pr(X_i = x_i).$$
We begin by observing that if the vector $v_i$ is the $0$ vector for any $i \in I$ then the set of vectors $V := \{v_i : i \in I\}$ fails to be linearly independent (modulo $2$), which contradicts our hypothesis. Hence for all $i \in I$, there exists $j \in [\ell]$ such that $v_{i,j} \neq 0$. Further from linear independence we observe that $\ell \geq k$.
\paragraph{}
On the one hand, for any $i \in I$, $v_i \neq 0$ and so
$$Pr(X_i = x_i) = \frac{1}{2}$$
since $X_i = x_i$ if and only if
$$P_i := \sum_{j \in s(v_i)} u_j = \begin{cases} \text{odd}, &\text{if $x_i = 1$} \\
\text{even}, &\text{otherwise}
\end{cases}$$
where $s(v_i) := \{j \in [\ell] : v_{i,j} \neq 0\}$. Note that since $s(v_i) \neq \emptyset$ and $u$ is chosen uniformly at random, the parity $P_i$ is chosen uniformly at random. Therefore
$$\Pi_{i \in I} Pr(X_i = x_i) = \Pi_{i \in I} \frac{1}{2} = \frac{1}{2^{|I|}}.$$
\paragraph{}
Now on the other hand, $\bigcap_{i \in I} X_i = x_i$ happens if and only if 
$$ Vu = x_I$$
where we define the matrix $V$ with $|I|$ rows and $\ell$ columns so that $V_{i,j} = v_{i,j}$, and $x_I$ is the vector $x$ restricted to entries indexed by $I$. Here we are taking all operations in the field $\F_2$. Since the rank of $V$ is $|I|$ we can find by Gaussian elimination a permuation matrix $E$ (after possibly permuting the columns and associated indices of $u$, without loss of generality) so that
$$EV = \begin{bmatrix}
I & B
\end{bmatrix}$$
for some binary matrix $B$ with $\ell - |I|$ columns. Then
$$\begin{bmatrix} I & B \end{bmatrix}u = Ex_I$$
and hence the choices of entries for $u_1, \dots, u_{|I|}$ are uniquely fixed given the choices of entries for $u_{|I| + 1}, \dots, u_{\ell}$. So there are $2^{\ell - |I|}$ choices for $u$ satisfying our matrix equation. There are $2^\ell$ total choices for $u$ overall, and so we have
$$Pr(\bigcap_{i \in I} X_i = x_i) = \frac{2^{\ell-|I|}}{2^\ell}  = \frac{1}{2^{|I|}} = \Pi_{i \in I} Pr(X_i = x_i)$$
as desired. $\blacksquare$
%Problem 2
\newpage
\section{}
\paragraph{}
Suppose we have $n$ items to be hashed into $n$ bins. Let $h$ be a hash function chosen from a $k$-universal family. We $\textit{overload}$ the notation ${n \choose k}$ to mean both the number of ways to choose $k$ elements from $n$, and the set of subsets of $[n]$ of size $k$. Which version we use will always be clear from context. Define a $k$-collision to be the event that $k$ distinct items are hashed to the same bin. For a $\textit{set}$ $S \in {n\choose k}$ let $X_S$ be the random indicator variable which is $1$ when $h(x) = h(y)$ for all $x,y \in S$, and $X_S$ is $0$ otherwise.
\paragraph{}
Let $X$ be a random variable counting the number of $k$-collisions when our $n$ items are hased using $h$ chosen from a $k$-universal family. Then $X = \sum_{S \in {n\choose k}} X_S$ and so we have
$$E[X] = E[\sum_{S \in {n\choose k}} X_S] = \sum_{S \in {n\choose k}} E[X_S] = \sum_{S \in {n \choose k}} Pr(|h(S)| = 1) \leq  \sum_{S \in {n \choose k}} \frac{1}{n^{k-1}}$$
where the inequality follows since $h$ is a $k$-universal hash function, and $S$ is a set of size $k$. So then
$$E[X] \leq {n\choose k} \frac{1}{n^{k-1}}.$$
Therefore if we let $t = 2n^{1-k}{n\choose k}$ then by Markov's inequality
$$Pr(X \geq t) \leq Pr(X \geq 2E[X]) \leq \frac{1}{2}.$$
\paragraph{}
Now let $Y$ be the maximum number of items hashed into a single bin. Then $X \geq {Y \choose k}$ and so
$$Pr({Y\choose k}\geq t) \leq Pr(X \geq t) \leq \frac{1}{2}$$
Which implies that 
$$Pr(Y \geq (2n)^{\frac{1}{k}}) \leq \frac{1}{2}.$$
That is, with probability at most $\frac{1}{2}$, the maximum load is larger than $(2n)^{\frac{1}{k}}$. 
\paragraph{}
We want to find the smallest $k$ for which 
$$Pr(Y \leq 3\ln n / \ln \ln n) \geq \frac{1}{2}.$$
Thus we need to choose the smallest $k$ satisfying
$$(2n)^{\frac{1}{k}} \leq \frac{3\ln n}{\ln \ln n}.$$
Rearranging we see that setting 
$$k \geq \frac{\ln2 + \ln n}{\ln(3) + \ln\ln n - \ln\ln\ln n}$$
would do the job. Hence the smallest choice of $k$ will be the first integer greater than this value, i.e.
$$k = \ceil{\frac{\ln2 + \ln n}{\ln(3) + \ln\ln n - \ln\ln\ln n}}.$$
$\blacksquare$
%Problem 3
\newpage
\section{}
I discussed this problem with Sharat.

\paragraph{}
Suppose we have a list $L$ of $n$ distinct numbers, whose sorted list would we $\{x_1,\dots, x_n\}$. We want to find a $k$-approximate median. I.e. we want to return some $x \in [x_{\frac{n}{2} - k}, x_{\frac{n}{2} + k}]\cap L$. Consider sampling $m$ numbers from $L$ and returning the median $c$ of these $m$ numbers as the true median of $L$.
\paragraph{}
Let $L'$ be list of $m$ sampled numbers. For each $i \in L'$ let $X_{i,<}$ be an indicator variable for the event that $i \in \{x_1, \dots, x_{\frac{n}{2} -k -1}\}$. Then, since we sample without replacement,
$$Pr(X_{i, <} =1) = (\frac{n}{2} -k -1) / n = \frac{n-2k-2}{2n}$$
Let $X_<$ be the number of elements in $L'$ which lie in $\{x_1, \dots, x_{\frac{n}{2} -k -1}\}$. Then $X_< = \sum_{i \in L'} X_{i,<}$.
Let $X_>$ be the number of elements in $L'$ which lie in $\{x_{\frac{n}{2} + k + 1}, \dots, x_n\}$. Since we sample uniformly and
	$$|\{x_{\frac{n}{2} + k + 1}e, \dots, x_n\}| =  |\{x_1, \dots, x_{\frac{n}{2} -k -1}\}| = \frac{n}{2} - k - 1$$
we observe that $X_<$ and $X_>$ have the same distribution.
\paragraph{}
Now we see that $c$ fails to be a $k$-approximate median for $L$ if and only if $X_< \geq \frac{m}{2}$ or $X_> \geq \frac{m}{2}$. Since $X_<$ and $X_>$ have the same distribution, the probability of failure is thus
$$2Pr(X_< \geq \frac{m}{2}).$$
We plan to bound this using a Chernoff bound. First we compute the expectation,
$$E[X_<] = \sum_{i\in L'} Pr(X_{i,<} = 1)  = \sum_{i \in L'} \frac{n-2k-2}{2n} = \frac{m(n-2(k+1))}{2n}.$$
So 
$$Pr(X_< \geq \frac{m}{2}) = Pr(X_< \geq (1+\delta) E[X_<])$$
where $$\delta =\frac{2(k+1)}{n-2(k+1)}.$$
We proceed by assuming $k < \frac{n}{4}-1$ so that $0<\delta <1$. Ultimately we are interested in results on how many samples $m$ we need to take to obtain a certain quality of approximation, and so bounding $k$ is reasonable.
\paragraph{}
Since $0 < \delta < 1$, and $X_<$ is a sum of independent random coin flips, we can apply the second Chernoff bound from Theorem on page $4$ of Lecture $2$, obtaining
$$Pr(X_< \geq \frac{m}{2}) < \exp(\frac{-\delta^2E[X_<]}{3}) = \exp(\frac{-2(k+1)^2m}{3n(n-2(k+1))}).$$
Hence the probability of failure is at most
$$2\exp(\frac{-2(k+1)^2m}{3n(n-2(k+1))})$$
To succeed with probability at least $0.9999$ we want this value to be at most $10^{-5}$. So it will suffice to choose $m$ and $k$ to satisfy
$$\exp(\frac{-2(k+1)^2m}{3n(n-2(k+1))}) \leq 10^{-6}$$
Rearranging yields the expression
$$(k+1)^2m \geq 9\ln(10) n^2 - 18\ln(10) n(k+1)$$
Dropping the non-positive term on the left-hand side, as this can only improve our probability of success, gives us the following expression for the trade-off between $m$ and $k$:
$$(k+1)^2m = 9\ln(10)n^2 = \Omega(n^2).$$
\paragraph{}
If we want $k \leq \epsilon n$ for small $\epsilon$ (say $<\frac{1}{4}$) then we need $m$ to satisfy
$$(\epsilon n+ 1)^2 m = 9\ln(10)n^2$$
i.e. 
$$m = \frac{9\ln(10)n^2}{(\epsilon n + 1)^2}.$$
\paragraph{}
Similarly if we want $k \leq n^{1-\eta}$ for $\eta \leq \frac{1}{2}$ then we take (again assuming desired $k$ is less than $\frac{1}{4}n-1$)
$$m =  \frac{9\ln(10)n^2}{(n^{1-\eta} + 1)^2}.$$
$\blacksquare$
%Problem 4
\newpage
\section{}
\paragraph{}
We assume that the determinant of an $n \times n$ matrix where each element is an entry of $\F[x]$ of degree at most $d$ can be computed in $O(dn^\omega)$ field operations.
\subsection{a}
Let $G=(U\cup V, E)$ be a bipartite graph where each edge is coloured either red or blue. We may assume $|V| = |U|$ by adding dummy vertices as needed. Let $k \in \Z$ be such that $1 \leq k \leq |E|$. We construct an $n\times n$ matrix $A$ of indeterminants as follows: for every edge $uv \in E$ set
$$A_{uv} = \begin{cases}
x_{uv}, \text{if $uv$ is coloured blue} \\
rx_{uv}, \text{if $uv$ is coloured red}
\end{cases}$$
with indeterminants $x_{uv}$ for each edge $uv$ and indeterminant $r$.
%Problem 5
\newpage
\section{}
\end{document}

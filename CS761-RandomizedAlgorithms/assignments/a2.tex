\documentclass[letterpaper,12pt,oneside,onecolumn]{article}
\usepackage[margin=1in, bottom=1in, top=1in]{geometry} %1 inch margins
\usepackage{amsmath, amssymb, amstext}
\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{theorem}
\usepackage{tikz}
\usepackage{tkz-berge}

%Macros
\newcommand{\A}{\mathbb{A}} \newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}} \newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}} \newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
 
 
\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\newenvironment{proof}{{\bf Proof:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofof}[1]{{\bf Proof of #1:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofofnobox}[1]{{\bf#1:  }}{}\newenvironment{example}{{\bf Example:  }}{\hfill\rule{2mm}{2mm}}

%\renewcommand{\thesection}{\lecnum.\arabic{section}}
%\renewcommand{\theequation}{\thesection.\arabic{equation}}
%\renewcommand{\thefigure}{\thesection.\arabic{figure}}

\newtheorem{fact}{Fact}[section]
\newtheorem{lemma}[fact]{Lemma}
\newtheorem{theorem}[fact]{Theorem}
\newtheorem{definition}[fact]{Definition}
\newtheorem{corollary}[fact]{Corollary}
\newtheorem{proposition}[fact]{Proposition}
\newtheorem{claim}[fact]{Claim}
\newtheorem{exercise}[fact]{Exercise}
\newtheorem{note}[fact]{Note}
\newtheorem{conjecture}[fact]{Conjecture}

\newcommand{\size}[1]{\ensuremath{\left|#1\right|}}
\newcommand{\ceil}[1]{\ensuremath{\left\lceil#1\right\rceil}}
\newcommand{\floor}[1]{\ensuremath{\left\lfloor#1\right\rfloor}}

%END MACROS
%Page style
\pagestyle{fancy}

\listfiles

\raggedbottom

\lhead{\today}
\rhead{W. Justin Toth CS761-Randomized Algorithms Assignment n2} %CHANGE n to ASSIGNMENT NUMBER ijk TO COURSE CODE
\renewcommand{\headrulewidth}{1pt} %heading underlined
%\renewcommand{\baselinestretch}{1.2} % 1.2 line spacing for legibility (optional)

\begin{document}
%Problem 1
\section{}
\paragraph{}
Let $v_1, \dots, v_m \in \{0,1\}^\ell$ be a set of vectors such that any subset of size at most $k$ is linearly independent modulo $2$. We denote the $j$-th entry of $v_i$ by $v_{i,j}$. Let $u$ be chosen uniformly at random from $\{0,1\}^\ell$. Let $X_i$ be the random variable defined by
$$X_i = (\sum_{j=1}^\ell v_{i,j}u_j) \mod 2.$$
We want to show that $X_1, \dots, X_m$ are $k$-wise independent random bits. That is to say, for all $I \subseteq [m]$ with $1\leq |I| \leq k$, and $x \in \{0,1\}^m$,
$$Pr(\bigcap_{i\in I} X_i = x_i) = \Pi_{i\in I} Pr(X_i = x_i).$$
We begin by observing that if the vector $v_i$ is the $0$ vector for any $i \in I$ then the set of vectors $V := \{v_i : i \in I\}$ fails to be linearly independent (modulo $2$), which contradicts our hypothesis. Hence for all $i \in I$, there exists $j \in [\ell]$ such that $v_{i,j} \neq 0$. Further from linear independence we observe that $\ell \geq k$.
\paragraph{}
On the one hand, for any $i \in I$, $v_i \neq 0$ and so
$$Pr(X_i = x_i) = \frac{1}{2}$$
since $X_i = x_i$ if and only if
$$P_i := \sum_{j \in s(v_i)} u_j = \begin{cases} \text{odd}, &\text{if $x_i = 1$} \\
\text{even}, &\text{otherwise}
\end{cases}$$
where $s(v_i) := \{j \in [\ell] : v_{i,j} \neq 0\}$. Note that since $s(v_i) \neq \emptyset$ and $u$ is chosen uniformly at random, the parity $P_i$ is chosen uniformly at random. Therefore
$$\Pi_{i \in I} Pr(X_i = x_i) = \Pi_{i \in I} \frac{1}{2} = \frac{1}{2^{|I|}}.$$
\paragraph{}
Now on the other hand, $\bigcap_{i \in I} X_i = x_i$ happens if and only if 
$$ Vu = x_I$$
where we define the matrix $V$ with $|I|$ rows and $\ell$ columns so that $V_{i,j} = v_{i,j}$, and $x_I$ is the vector $x$ restricted to entries indexed by $I$. Here we are taking all operations in the field $\F_2$. Since the rank of $V$ is $|I|$ we can find by Gaussian elimination a permuation matrix $E$ (after possibly permuting the columns and associated indices of $u$, without loss of generality) so that
$$EV = \begin{bmatrix}
I & B
\end{bmatrix}$$
for some binary matrix $B$ with $\ell - |I|$ columns. Then
$$\begin{bmatrix} I & B \end{bmatrix}u = Ex_I$$
and hence the choices of entries for $u_1, \dots, u_{|I|}$ are uniquely fixed given the choices of entries for $u_{|I| + 1}, \dots, u_{\ell}$. So there are $2^{\ell - |I|}$ choices for $u$ satisfying our matrix equation. There are $2^\ell$ total choices for $u$ overall, and so we have
$$Pr(\bigcap_{i \in I} X_i = x_i) = \frac{2^{\ell-|I|}}{2^\ell}  = \frac{1}{2^{|I|}} = \Pi_{i \in I} Pr(X_i = x_i)$$
as desired. $\blacksquare$
%Problem 2
\newpage
\section{}
\paragraph{}
Suppose we have $n$ items to be hashed into $n$ bins. Let $h$ be a hash function chosen from a $k$-universal family. We $\textit{overload}$ the notation ${n \choose k}$ to mean both the number of ways to choose $k$ elements from $n$, and the set of subsets of $[n]$ of size $k$. Which version we use will always be clear from context. Define a $k$-collision to be the event that $k$ distinct items are hashed to the same bin. For a $\textit{set}$ $S \in {n\choose k}$ let $X_S$ be the random indicator variable which is $1$ when $h(x) = h(y)$ for all $x,y \in S$, and $X_S$ is $0$ otherwise.
\paragraph{}
Let $X$ be a random variable counting the number of $k$-collisions when our $n$ items are hased using $h$ chosen from a $k$-universal family. Then $X = \sum_{S \in {n\choose k}} X_S$ and so we have
$$E[X] = E[\sum_{S \in {n\choose k}} X_S] = \sum_{S \in {n\choose k}} E[X_S] = \sum_{S \in {n \choose k}} Pr(|h(S)| = 1) \leq  \sum_{S \in {n \choose k}} \frac{1}{n^{k-1}}$$
where the inequality follows since $h$ is a $k$-universal hash function, and $S$ is a set of size $k$.
%Problem 3
\newpage
\section{}

%Problem 4
\newpage
\section{}

%Problem 5
\newpage
\section{}
\end{document}

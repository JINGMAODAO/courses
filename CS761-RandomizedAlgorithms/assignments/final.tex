\documentclass[letterpaper,12pt,oneside,onecolumn]{article}
\usepackage[margin=1in, bottom=1in, top=1in]{geometry} %1 inch margins
\usepackage{amsmath, amssymb, amstext}
\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{theorem}
\usepackage{tikz}
\usepackage{tkz-berge}

%Macros
\newcommand{\A}{\mathbb{A}} \newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}} \newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}} \newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
 
 
\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\newenvironment{proof}{{\bf Proof:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofof}[1]{{\bf Proof of #1:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofofnobox}[1]{{\bf#1:  }}{}\newenvironment{example}{{\bf Example:  }}{\hfill\rule{2mm}{2mm}}

%\renewcommand{\thesection}{\lecnum.\arabic{section}}
%\renewcommand{\theequation}{\thesection.\arabic{equation}}
%\renewcommand{\thefigure}{\thesection.\arabic{figure}}

\newtheorem{fact}{Fact}[section]
\newtheorem{lemma}[fact]{Lemma}
\newtheorem{theorem}[fact]{Theorem}
\newtheorem{definition}[fact]{Definition}
\newtheorem{corollary}[fact]{Corollary}
\newtheorem{proposition}[fact]{Proposition}
\newtheorem{claim}[fact]{Claim}
\newtheorem{exercise}[fact]{Exercise}
\newtheorem{note}[fact]{Note}
\newtheorem{conjecture}[fact]{Conjecture}

\newcommand{\size}[1]{\ensuremath{\left|#1\right|}}
\newcommand{\ceil}[1]{\ensuremath{\left\lceil#1\right\rceil}}
\newcommand{\floor}[1]{\ensuremath{\left\lfloor#1\right\rfloor}}

%END MACROS
%Page style
\pagestyle{fancy}

\listfiles

\raggedbottom

\lhead{\today}
\rhead{W. Justin Toth CS761-Randomized Algorithms Final Exam} %CHANGE n to ASSIGNMENT NUMBER ijk TO COURSE CODE
\renewcommand{\headrulewidth}{1pt} %heading underlined
%\renewcommand{\baselinestretch}{1.2} % 1.2 line spacing for legibility (optional)

\begin{document}

%Problem 1
\section{}
\paragraph{}
Denote the applicants $A_1, \dots, A_n$ in the order which they appear according to the random permutation. Let $X_i$ be the $0$-$1$ indicator random variable for the event that we pay penalty $C$ for  $A_i$. Hence $X_i=1$ if and only if we hire $A_i$ and later fire $A_i$.
\paragraph{}
We hire $A_i$ if and only if $A_i$ is ranked highest among the first $A_1, \dots, A_i$ applicants. This happens with probability
$$\frac{1}{i}$$
since the ordering was chosen uniformly at random. Given that we hire $A_i$, we fire $A_i$ if and only if $A_i$ is not ranked highest among the following $A_i, \dots, A_n$ applicants. This happens with probability
$$\frac{n-i}{n-i+1}.$$
Since the events that $A_i$ is ranked first among the first $i$ candidates, and not ranked first among the last $n-i+1$ candidates are independent we have
$$Pr(X_i) = \frac{n-i}{i(n-i+1)}.$$
\paragraph{}
Thus our expected total penalty is
$$\sum_{i=1}^n C\cdot Pr(X_i) = C \sum_{i=1}^n\frac{n-i}{i(n-i+1)} = CS_n$$
where $S_n$ denotes $\sum_{i=1}^n\frac{n-i}{i(n-i+1)}$. Now observe that
$$S_n = n\sum_{i=1}^n\frac{1}{i(n-i+1)} - \sum_{i=1}^n\frac{1}{n-i+1}= n\sum_{i=1}^n\frac{1}{i(n-i+1)} - H_n =  n\sum_{i=1}^n\frac{1}{i(n-i+1)} - \log(n)$$
where $H_n$ denotes the $n$-th harmonic number. Now since
$$\int_{x=1}^n \frac{1}{x(n-x+1)} dx = \frac{2\log(n)}{n+1}$$
we have that
$$S_n \leq \frac{2n\log(n)}{n+1} -\log(n) = \frac{(n-1)\log(n)}{n+1}.$$
Therefore our expected total payoff is at most
$$C\cdot \frac{(n-1)\log(n)}{n+1}. \blacksquare$$
%Problem 2
\newpage
\section{}
Let $G$ be an undirected graph, and $C$ the size of a minimum cut in $G$. Let $\alpha \in \R_{\geq 1}$.
\begin{lemma}\label{lemma:2}
Let $F \subseteq E(G)$ be a cut such that $|F| \leq \alpha C$. Then the probability that $F$ is output by Karger's random contraction algorithm is at least
$$\Omega({\frac{1}{n^{2\alpha}}}).$$
\end{lemma}
\begin{proof}
We say the algorithm ``succeeds" if $F$ is output by the algorithm. The algorithm succeeds if and only if we never contract an edge of $F$. At each iteration of the algorithm, every subset of vertices in the contracted graph corresponds to a subset of vertices  in the original graph (by expanding contracted vertices), and hence the minimum cut value of contracted graph at iteration $i$ is at least $C$.
\paragraph{}
In particular the degree of each vertex in the contracted graph is at least $C$. So the number of edges in the $i$-th iteration is at least
$$\frac{(n-i+1)C}{2}.$$
So when we pick a random edge to contract in this iteration, the probability that we pick an edge of $F$ is at most
$$\frac{|F|}{\frac{(n-i+1)C}{2}} \leq \frac{2\alpha}{n-i+1}$$
using that $|F| \leq \alpha C.$ So the probability that the algorithm succeeds is at least
$$\Pi_{i=1}^n(1-\frac{2\alpha}{n-i+1}) = \frac{2\alpha!}{n(n-1)\dots(n-2\alpha + 1)} = \Omega(\frac{1}{n^{2\alpha}})$$
\end{proof}
\begin{claim}\label{claim2}
The number of cuts in $G$ with at most $\alpha C$ edges is at most $O(n^{2\alpha})$.
\end{claim}
\begin{proof}
Let $\cF$ be the set of cuts in $G$ with at most $\alpha C$ edges. For $F \in \cF$ let $X_F$ be the event that Karger's algorithm outputs $F$. By Lemma \ref{lemma:2}
$$Pr(X_F) \geq \Omega(\frac{1}{n^{2\alpha}}).$$
Now we see that, one the one hand
$$Pr(\bigcup_{F \in \cF} X_F) \leq 1$$
and on the other hand, as the events events $X_F$ are all disjoint (the algorithm only outputs one answer),
$$Pr(\bigcup_{F \in \cF} X_F) = \sum_{F \in \cF} Pr(X_F) \leq \sum_{F \in \cF} \Omega(\frac{1}{n^{2\alpha}}) = |\cF|\Omega(\frac{1}{n^{2\alpha}}).$$
Combining inequality and rearranging for $|\cF|$ we have
$$|\cF| \leq O(n^{2\alpha})$$
as desired.
\end{proof}
\paragraph{}
If we repeat Karger's algorithm $t$ times waiting for a particular cut $F \in \cF$ (defined in above proof) we have a failure probability at most
$$(1-\Omega(\frac{1}{n^{2\alpha}}))^t \leq \exp(\frac{t}{O(n^{2{\alpha}})})$$
and hence we can make this failure probability small with a choice of $t \in O(n^{2\alpha})$ (the constant would depend on how good you want the success probability).
\paragraph{}
So we give the following randomized algorithm for enumerating all cuts of $G$ with at most $\alpha C$ edges. Let $k$ be some parameter to be chosen later. Our algorithm proceeds as follows:
\begin{enumerate}
\item Let $\cL = \emptyset$.
\item While fail$<k$
	\begin{enumerate}
	\item Repeat Karger's algorithm at most $t$ times hoping for a cut $F\in \cF\backslash \cL$.
	\item If such $F$ is found, add $F$ to $\cL$ and set fail$=0$.
	\item Otherwise set fail$=$fail$+1$.
	\end{enumerate}
\item Output $\cL$.
\end{enumerate}
\paragraph{}
In words the algorithm uses the boosted version of Karger's to try to find a cut with at most $\alpha C$ edges that it has not seen before. It stops if it fails to find such a cut $k$ times in a row, and prints all the cuts found this way.
\paragraph{}
The algorithm fails if in some $i$-th round it does not find a cut $F \in \cF\backslash \cL$ $k$ times in a row, while $\cF \backslash \cL\neq \emptyset$. We know from \ref{claim2} there are at most $O(n^{2\alpha})$ such rounds $i$. Let $X_i$ be a random variable indicating failure in the $i$-th round. We can choose $t$ so that $t$ repetitions of Karger's algorithm finds $F \in \cF\backslash \cL$ (provided such $F$ exists) with high probability, say $0.99$.
\paragraph{}
Then the probability that we fail in the $i$-th round is
$$(0.01)^k.$$
So the probability we fail in some round is at most
$$\frac{O(n^{2\alpha})}{100^k}.$$
Since $\alpha$ is constant, we can choose $k \in O(\log n)$ to make this failure probability arbitrarily small. Let $T$ be the time to run Karger's algorithm once.  Since there are $O(n^{2\alpha})$ rounds our algorithm takes
$$O(n^{2\alpha}ktT) = O(n^{4\alpha}\log n T)$$
time.$ \blacksquare$

%Problem 3
\newpage
\section{}
\paragraph{}
We throw $n^{0.99}$ balls into $n$ bins uniformly at random. Using union bound, the probability that a bin has at least $k$ balls, for $k$ chosen later, is at most 
$${n \choose k}(\frac{1}{n})^k.$$
Using the bound ${n^{0.99}\choose k} <(\frac{n^{0.99}e}{k})^k$ this probability is at most
$$(\frac{n^{0.99}e}{k})^k(\frac{1}{n})^k= \frac{e^k}{n^{0.0k}k^k}\leq \frac{e^k}{k^k}$$
where the last inequality follows since we will choose $k \geq 0$ and since $n\geq 1$ this implies $n^\frac{k}{100} \geq 1$.
Again invoking union bound
$$Pr(\exists\text{a bin with at least $k$ balls}) \leq n\frac{e^k}{k^k} = n(\frac{e}{k})^k.$$ 
Since we want
$$Pr(\exists\text{a bin with at least $k$ balls}) \leq \frac{1}{2}$$
we need to find $k$ such that
$$n(\frac{e}{k})^k \leq \frac{1}{2}.$$

%Problem 4
\newpage
\section{}
\paragraph{}
We always use addition, $+$, over the appropriate field (ie. $\F_n$ for inputs and $\F_m$ for outputs). Our algorithm is very simple. Given input $z$ choose $x \in \{0,\dots, n-1\}$ uniformly at random. Let $y = z-x$. Lookup $F(x)$ and $F(y)$ and return $F(x) + F(y)$.
\paragraph{}
If both $F(x)$ and $F(y)$ are not modified by our adversary then our return value satisfies
$$F(x) + F(y) = F(x+y) = F(z)$$
using the linearity of $F$ and the fact that $x+y = z$. So a necessary condition for our algorithm to fail is that one of $F(x)$ or $F(y)$ was modified by our adversary. For a uniform random element of the table, the probability of modification is at most $\frac{1}{5}$. Hence
$$Pr(F(x)\text{ or } F(y)\text{ was modified}) \leq Pr(F(x) \text{ was modified}) + Pr(F(y)\text{ was modified}) \leq \frac{2}{5}.$$
Therefore the probability of failure is at most
$$\frac{2}{5} < \frac{1}{2}.$$
Hence our algorithm succeeds with probability at least $\frac{1}{2}$ as desired. $\blacksquare$

%Problem 5
\newpage
\section{}
\paragraph{}
Let $S_1$ and $S_2$ be multisets of positive integers. Let $|S_1| = |S_2| = n$ be the number of elements in each multiset. If the sets are of different sizes it is trivial to decide they are not equal. Let $h:[n] \rightarrow [cn]$ be a $2$-universal hash function chosen uniformly at random.
\paragraph{}
We're going to define multiset operations. They are exactly what you'd expect them to be, so feel free to skip this paragraph as I only add them for sake of completeness. Let $S_1\cap S_2$ denote the largest multiset that is a sub-multiset of $S_1$ and a sub-multiset of $S_2$. By sub-multiset I mean $A$ is a sub-multiset of $B$ ($A\subseteq B$) if and only if for every element $e$ of $A$, the number of times $e$ appears in $A$ is at most the number of times $e$ appears in $B$. We define multiset addition so that $A+B$ is the multiset of elements $e$ so that $e$ appears as many times as it does in $A$ plus as many times as it does in $B$. We also need multiset difference. For multisets $A$ and $B$ we define $A-B$ to be the unique multiset so to that $(A-B) + (A\cap B) = A$. My multiset equality, $A=B$, we meant that $A$ and $B$ have precisely the same elements appearing the same number of times.
\paragraph{}
If $S_1 = S_2$ then $h(S_1) = h(S_2)$ and so the algorithm never gives false negatives. Observe that $h(S_1) = h(S_1\cap S_2) + h(S_1-S_2)$ and $h(S_2) = h(S_1\cap S_2) + h(S_2-S_1)$. Hence the algorithm can only give a false positive if $h(S_1-S_2) = h(S_2 - S_1)$. We will bound the probability of failure by the probability of this happening. Let $d = |S_1-S_2| =|S_2-S_1|$. For $a \in S_1 - S_2$ and $b \in S_2 - S_1$ define $X_{a,b}$ to be the event that $h(a) = h(b)$. Further for each $a \in S_1 - S_2$ define
$$X_a  = \bigcup_{b \in S_2 - S_1} X_{a,b}.$$
Notice that $X_a$ is the disjoint union of events $X_{a,b}$. Let $X$ be a random variable counting the number of collisions between elements in $S_1-S_2$ and elements in $S_2-S_1$. We have
$$E[X] = \sum_{a \in S_1 - S_2} E[X_a] = \sum_{a\in S_1 - S_2} \sum_{b \in S_2-S_1} Pr(h(a) = h(b)) \leq  \sum_{a\in S_1 - S_2} \sum_{b \in S_2-S_1} \frac{1}{cn} = \frac{d^2}{cn}.$$
The inequality follows from $h$ being $2$-universal. For the algorithm to fail it is necessary to have $X \geq d$, for otherwise there are an insufficient number of collisions to have $h(S_1-S_2) = h(S_2-S_1)$. Therefore we can bound the probability of failure by $Pr(X  \geq d)$. By Markov's inequality we see that,
$$Pr(X \geq d) \leq \frac{E[X]}{d} \leq \frac{d}{cn}.$$
Hence our probability of failure is at most $\frac{d}{cn}$. Assuming we can hash in constant time and lookup in a hash table in constant time, the algorithm takes $O(n)$ time to hash each element in $S_1$ and $S_2$, then takes $O(cn)$ time to verify the $cn$ counters each associated with an entry of each of the two hash tables. Since we need $c \geq 1$ to avoid guaranteed collisions, this implies the algorithm runs in $O(cn)$ time with failure probability at most $\frac{d}{cn}$.
\paragraph{}
In the previous paragraph we demonstrated a trade-off between running time and failure probability with respect to choice of parameter $c$. To obtain a Monte Carlo algorithm, we need a failure probability which is independent of the input size. We make our extension by giving an explicit choice of $c$. Let $p \in (0,1)$. Then if we choose $c = \ceil{\frac{d}{pn}}$ then we obtain a failure probability at most $p$ and a running time $O(pd)$. $\blacksquare$
%Problem 6
\newpage
\section{}
\paragraph{}
Let $\cA$ be an algorithm which obtain an $(\epsilon, \frac{1}{4})$-approximation to some parameter $\mu$. That is, when $X$ is the output value of $\cA$,
$$Pr(|X - \mu| \leq \epsilon\mu) \leq \frac{1}{4}.$$
We want to show how to obtain an $(\epsilon,\delta)$-approximation to $\mu$. Suppose we perform $k = \ceil{\log(\frac{1}{\delta})}$ independent runs of $\cA$ and output the median. We may assume $k$ is even, if not replace $k$ with $k+1$. Let $X_1, \dots, X_k$ be random variables each denoting the output value of one run of $\cA$. Instead of ordering them by the order the trials happen, order them sorted by value, i.e.
$$X_1 \leq X_2 \leq \dots\leq X_k.$$
Our $(\epsilon,\delta)$-approximation algorithm outputs $X_\frac{k}{2}$, the median value of our independent trials. Let $X$ denote the output of our new algorithm. We observe that $|X - \mu|\leq \epsilon \mu$ if and only if 
$$|X_i - \mu | \leq \epsilon \mu, \quad \forall i=1 \dots \frac{k}{2}.$$
Therefore
$$Pr(|X-\mu| \leq \epsilon \mu) = \Pi_{i=1}^\frac{k}{2} Pr(|X_i - \mu| \leq \epsilon \mu) \leq \frac{1}{4^\frac{k}{2}}.$$
Since $k = \ceil{\log(\frac{1}{\delta})}$,
$$\frac{1}{4^\frac{k}{2}} \leq \frac{1}{4^{\frac{1}{2}\log(\frac{1}{\delta})}} = \delta.$$
Therefore
$$Pr(|X-\mu| \leq \epsilon \mu) \leq \delta. \blacksquare$$
%Problem 7
\newpage
\section{}

%Problem 8
\newpage
\section{}

%Problem 9
\newpage
\section{}

%Problem 10
\newpage
\section{}

\end{document}
